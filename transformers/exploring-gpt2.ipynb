{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring GPT-2\n",
    "\n",
    "In this Jupyter Notebook, we are exploring the GPT-2 model from Huggingface. This is inspired by [Andrej Karpathy's recent YT video](https://www.youtube.com/watch?v=l8pRSuU81PU&t=2593s). However, I will not try to match the structure defined by huggingface model and try to make names a bit more descriptive. So, let's start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kapilsharma/.conda/envs/ml-projects/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import pipeline, set_seed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the huggingface GPT2 transformers model for a spin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kapilsharma/.conda/envs/ml-projects/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== encoded input ===\n",
      "{'input_ids': tensor([[3041, 5372,  502,  416,  597, 2420,  345, 1549,  588,   13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "print(\"=== encoded input ===\")\n",
    "print(encoded_input)\n",
    "_ = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the model directly for some text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kapilsharma/.conda/envs/ml-projects/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, but what I'm really doing is making a human-readable document. There are other languages, but those are\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a syntax model. That's why I like it. I've done a lot of programming projects.\\n\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and I'll do it in no time!\\n\\nOne of the things we learned from talking to my friend\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a command line tool.\\n\\nIf my code is simple enough:\\n\\nif (use (string\"},\n",
       " {'generated_text': \"Hello, I'm a language model, I've been using Language in all my work. Just a small example, let's see a simplified example.\"}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the raw parameter dimensions from the published model on huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight torch.Size([50257, 768])\n",
      "transformer.wpe.weight torch.Size([1024, 768])\n",
      "transformer.h.0.ln_1.weight torch.Size([768])\n",
      "transformer.h.0.ln_1.bias torch.Size([768])\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.0.ln_2.weight torch.Size([768])\n",
      "transformer.h.0.ln_2.bias torch.Size([768])\n",
      "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_1.weight torch.Size([768])\n",
      "transformer.h.1.ln_1.bias torch.Size([768])\n",
      "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_2.weight torch.Size([768])\n",
      "transformer.h.1.ln_2.bias torch.Size([768])\n",
      "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_1.weight torch.Size([768])\n",
      "transformer.h.2.ln_1.bias torch.Size([768])\n",
      "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_2.weight torch.Size([768])\n",
      "transformer.h.2.ln_2.bias torch.Size([768])\n",
      "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_1.weight torch.Size([768])\n",
      "transformer.h.3.ln_1.bias torch.Size([768])\n",
      "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_2.weight torch.Size([768])\n",
      "transformer.h.3.ln_2.bias torch.Size([768])\n",
      "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_1.weight torch.Size([768])\n",
      "transformer.h.4.ln_1.bias torch.Size([768])\n",
      "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_2.weight torch.Size([768])\n",
      "transformer.h.4.ln_2.bias torch.Size([768])\n",
      "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_1.weight torch.Size([768])\n",
      "transformer.h.5.ln_1.bias torch.Size([768])\n",
      "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_2.weight torch.Size([768])\n",
      "transformer.h.5.ln_2.bias torch.Size([768])\n",
      "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_1.weight torch.Size([768])\n",
      "transformer.h.6.ln_1.bias torch.Size([768])\n",
      "transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_2.weight torch.Size([768])\n",
      "transformer.h.6.ln_2.bias torch.Size([768])\n",
      "transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_1.weight torch.Size([768])\n",
      "transformer.h.7.ln_1.bias torch.Size([768])\n",
      "transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_2.weight torch.Size([768])\n",
      "transformer.h.7.ln_2.bias torch.Size([768])\n",
      "transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_1.weight torch.Size([768])\n",
      "transformer.h.8.ln_1.bias torch.Size([768])\n",
      "transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_2.weight torch.Size([768])\n",
      "transformer.h.8.ln_2.bias torch.Size([768])\n",
      "transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_1.weight torch.Size([768])\n",
      "transformer.h.9.ln_1.bias torch.Size([768])\n",
      "transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_2.weight torch.Size([768])\n",
      "transformer.h.9.ln_2.bias torch.Size([768])\n",
      "transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_1.weight torch.Size([768])\n",
      "transformer.h.10.ln_1.bias torch.Size([768])\n",
      "transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_2.weight torch.Size([768])\n",
      "transformer.h.10.ln_2.bias torch.Size([768])\n",
      "transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_1.weight torch.Size([768])\n",
      "transformer.h.11.ln_1.bias torch.Size([768])\n",
      "transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_2.weight torch.Size([768])\n",
      "transformer.h.11.ln_2.bias torch.Size([768])\n",
      "transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.ln_f.weight torch.Size([768])\n",
      "transformer.ln_f.bias torch.Size([768])\n",
      "lm_head.weight torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "parameters_state_dict = model.state_dict()\n",
    "for key, value in parameters_state_dict.items():\n",
    "    print(key, value.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2 Architecture\n",
    "\n",
    "In the above architecture, we can identify different layers as:\n",
    "\n",
    "- wte and wpe are the token and position embeddings, respectively\n",
    "- h.<N> represents attention blocks. Each attention block has 4 sublayers\n",
    "    -  ln_1: LayerNorm 1 (i.e. pre attention). Note that GPT2 changed the order of LayerNorm in the decoder layer to move it before attention and MLP\n",
    "    - attn: attention layer\n",
    "    - ln_2: LayerNorm 2 (i.e. pre MLP)\n",
    "    - mlp: Multi Layer Perceptron in each decoder block\n",
    "- ln_f is the final LayerNorm that is added after all the decoder attention blocks\n",
    "\n",
    "Extract from GPT-2 paper that highlights the changes:\n",
    "\n",
    "![GPT2 Layer Norm Modification](./gpt2-layer-norm-modification.png)\n",
    "\n",
    "Based on the description, we can generate the GPT-2 architecture diagram\n",
    "\n",
    "<img src=\"./gpt2.png\" alt=\"GPT-2 Architecture\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the GPT-2 PyTorch model from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "Let's start defining the model layer by layer. Initially, we will create a model config that has all the hyper parameters for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPT2Config:\n",
    "    vocab_size: int = 50257\n",
    "    context_window_size: int = 1024\n",
    "    embedding_dimension: int = 768\n",
    "    num_layers: int = 12\n",
    "    num_heads: int = 12\n",
    "    dropout_embeddings: float = 0.1\n",
    "    dropout_attention: float = 0.1\n",
    "    dropout_residual: float = 0.1\n",
    "    layer_norm_epsilon: float = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above config, let us define the GPT2 model skeleton:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2(nn.Module):\n",
    "    def __init__(self, config: GPT2Config, device: str = 'cpu'):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size,\n",
    "                                            config.embedding_dimension)\n",
    "        self.positional_embedding = nn.Embedding(config.context_window_size,\n",
    "                                                 config.embedding_dimension)\n",
    "        self.positional_ids = torch.arange(\n",
    "            config.context_window_size).unsqueeze(0).to(device)\n",
    "        self.embedding_dropout = nn.Dropout(config.dropout_embeddings)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [GPT2Layer(config) for _ in range(config.num_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(config.embedding_dimension)\n",
    "        # gpt2 does not use a bias in the output layer\n",
    "        self.output_layer = nn.Linear(config.embedding_dimension,\n",
    "                                      config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor):\n",
    "        _, current_sequence_length = input_ids.size()\n",
    "        positions = self.positional_ids[:, :current_sequence_length]\n",
    "        embeddings = self.token_embedding(\n",
    "            input_ids) + self.positional_embedding(positions)\n",
    "        embeddings = self.embedding_dropout(embeddings)\n",
    "        for layer in self.layers:\n",
    "            embeddings = layer(embeddings)\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        return self.output_layer(embeddings)  # returns logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPT2 model consists of token embeddings, positional embeddings, multiple GPT2 layers, layer normalization, and an output layer. GPT2 model takes input_ids as input and performs the following steps:\n",
    "  - Embeds the input_ids using token embeddings and positional embeddings.\n",
    "  - Applies dropout to regularize the embeddings before applying attention\n",
    "  - Passes the embeddings through each GPT2 layer.\n",
    "  - Applies layer normalization to the final embeddings.\n",
    "  - Passes the normalized embeddings through the output layer to get logits.\n",
    "\n",
    "We are still to define the GPT2Layer. Let's do that now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Layer(nn.Module):\n",
    "    def __init__(self, config: GPT2Config):\n",
    "        super().__init__()\n",
    "        self.layer_norm1 = nn.LayerNorm(config.embedding_dimension)\n",
    "        self.attention = CausalMultiHeadAttention(config)\n",
    "        self.layer_norm2 = nn.LayerNorm(config.embedding_dimension)\n",
    "        self.residual_dropout = nn.Dropout(config.dropout_residual)\n",
    "        self.mlp = MLP(config)\n",
    "        self.dropout_mlp = nn.Dropout(config.dropout_residual)\n",
    "\n",
    "    def forward(self, embeddings: torch.Tensor):\n",
    "        attention_output = embeddings + self.attention(\n",
    "            self.layer_norm1(embeddings))\n",
    "        attention_output = self.residual_dropout(attention_output)\n",
    "        mlp_output = attention_output + self.mlp(\n",
    "            self.layer_norm2(attention_output))\n",
    "        return mlp_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In GPT2Layer, input embeddings are passed through a layer normalization layer, then through the self-attention mechanism, and finally through another layer normalization layer. \n",
    "\n",
    "The output of the self-attention mechanism is added to the input embeddings to form the attention_output as a residual connenction. The attention_output is then passed through another layer normalization layer and a multi-layer perceptron (MLP) to form the mlp_output. The mlp_output is then returned.\n",
    "\n",
    "Let's define the MLP module first since that is the simpler of the two modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: GPT2Config):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.embedding_dimension,\n",
    "                             4 * config.embedding_dimension)\n",
    "        # the approximated gelu is only needed to match the outputs perfectly with the huggingface model\n",
    "        self.activation = nn.GELU(approximate=\"tanh\")\n",
    "\n",
    "        self.fc2 = nn.Linear(4 * config.embedding_dimension,\n",
    "                             config.embedding_dimension)\n",
    "        self.dropout = nn.Dropout(config.dropout_residual)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.dropout(self.fc2(self.activation(self.fc1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: We use the approximated gelu to match the outputs perfectly with the huggingface model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty straight-forward. It is a 2 layer MLP with a GELU activation in between. For more details on GELU, go to original paper - [GAUSSIAN ERROR LINEAR UNITS](https://arxiv.org/pdf/1606.08415v5).\n",
    "\n",
    "Now, let's implement the final piece - `CausalMultiHeadAttention`. For attention calculation, we will use the flash attention implementation provided by pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config: GPT2Config):\n",
    "        super().__init__()\n",
    "        self.num_heads = config.num_heads\n",
    "        self.embedding_dimension = config.embedding_dimension\n",
    "        self.head_dim = self.embedding_dimension // self.num_heads\n",
    "        assert self.head_dim * self.num_heads == self.embedding_dimension, \\\n",
    "            \"embedding_dimension must be divisible by num_heads\"\n",
    "        self.qkv = nn.Linear(self.embedding_dimension,\n",
    "                             3 * self.embedding_dimension)\n",
    "        self.out = nn.Linear(self.embedding_dimension, self.embedding_dimension)\n",
    "        self.prob_dropout = config.dropout_attention\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        batch_size, sequence_length, _ = x.size()\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.split(self.embedding_dimension,\n",
    "                            dim=2)  # split along the third dimension\n",
    "\n",
    "        k = k.view(batch_size, sequence_length, self.num_heads,\n",
    "                   self.head_dim).permute(0, 2, 1, 3)\n",
    "        v = v.view(batch_size, sequence_length, self.num_heads,\n",
    "                   self.head_dim).permute(0, 2, 1, 3)\n",
    "        q = q.view(batch_size, sequence_length, self.num_heads,\n",
    "                   self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        weights = F.scaled_dot_product_attention(\n",
    "            q, k, v, is_causal=True,\n",
    "            dropout_p=self.prob_dropout if self.training else 0)\n",
    "\n",
    "        output = weights.transpose(1, 2).contiguous().view(\n",
    "            batch_size, sequence_length, self.embedding_dimension)\n",
    "        output = self.out(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we should be at least be able to call the GPT2 model and get some output. The weights are randomly initialized so the output will be garbage, but we will tackle that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 50257])\n"
     ]
    }
   ],
   "source": [
    "gpt2_model = GPT2(GPT2Config())\n",
    "gpt2_out = gpt2_model(encoded_input['input_ids'])\n",
    "print(gpt2_out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! so we are able to call the model e2e. \n",
    "\n",
    "Now starts the fun part. At this point, we should be able to take the weights from the huggingface model and apply them to the model we just defined.\n",
    "\n",
    "Let's first print all the parameter keys and dimensions and match it against the huggingface model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_embedding.weight torch.Size([50257, 768]) <> transformer.wte.weight torch.Size([50257, 768])\n",
      "positional_embedding.weight torch.Size([1024, 768]) <> transformer.wpe.weight torch.Size([1024, 768])\n",
      "layers.0.layer_norm1.weight torch.Size([768]) <> transformer.h.0.ln_1.weight torch.Size([768])\n",
      "layers.0.layer_norm1.bias torch.Size([768]) <> transformer.h.0.ln_1.bias torch.Size([768])\n",
      "layers.0.attention.qkv.weight torch.Size([2304, 768]) <> transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
      "layers.0.attention.qkv.bias torch.Size([2304]) <> transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
      "layers.0.attention.out.weight torch.Size([768, 768]) <> transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
      "layers.0.attention.out.bias torch.Size([768]) <> transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
      "layers.0.layer_norm2.weight torch.Size([768]) <> transformer.h.0.ln_2.weight torch.Size([768])\n",
      "layers.0.layer_norm2.bias torch.Size([768]) <> transformer.h.0.ln_2.bias torch.Size([768])\n",
      "layers.0.mlp.fc1.weight torch.Size([3072, 768]) <> transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "layers.0.mlp.fc1.bias torch.Size([3072]) <> transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
      "layers.0.mlp.fc2.weight torch.Size([768, 3072]) <> transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "layers.0.mlp.fc2.bias torch.Size([768]) <> transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
      "layers.1.layer_norm1.weight torch.Size([768]) <> transformer.h.1.ln_1.weight torch.Size([768])\n",
      "layers.1.layer_norm1.bias torch.Size([768]) <> transformer.h.1.ln_1.bias torch.Size([768])\n",
      "layers.1.attention.qkv.weight torch.Size([2304, 768]) <> transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
      "layers.1.attention.qkv.bias torch.Size([2304]) <> transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
      "layers.1.attention.out.weight torch.Size([768, 768]) <> transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
      "layers.1.attention.out.bias torch.Size([768]) <> transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
      "layers.1.layer_norm2.weight torch.Size([768]) <> transformer.h.1.ln_2.weight torch.Size([768])\n",
      "layers.1.layer_norm2.bias torch.Size([768]) <> transformer.h.1.ln_2.bias torch.Size([768])\n",
      "layers.1.mlp.fc1.weight torch.Size([3072, 768]) <> transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "layers.1.mlp.fc1.bias torch.Size([3072]) <> transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
      "layers.1.mlp.fc2.weight torch.Size([768, 3072]) <> transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "layers.1.mlp.fc2.bias torch.Size([768]) <> transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
      "layers.2.layer_norm1.weight torch.Size([768]) <> transformer.h.2.ln_1.weight torch.Size([768])\n",
      "layers.2.layer_norm1.bias torch.Size([768]) <> transformer.h.2.ln_1.bias torch.Size([768])\n",
      "layers.2.attention.qkv.weight torch.Size([2304, 768]) <> transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
      "layers.2.attention.qkv.bias torch.Size([2304]) <> transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
      "layers.2.attention.out.weight torch.Size([768, 768]) <> transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
      "layers.2.attention.out.bias torch.Size([768]) <> transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
      "layers.2.layer_norm2.weight torch.Size([768]) <> transformer.h.2.ln_2.weight torch.Size([768])\n",
      "layers.2.layer_norm2.bias torch.Size([768]) <> transformer.h.2.ln_2.bias torch.Size([768])\n",
      "layers.2.mlp.fc1.weight torch.Size([3072, 768]) <> transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "layers.2.mlp.fc1.bias torch.Size([3072]) <> transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
      "layers.2.mlp.fc2.weight torch.Size([768, 3072]) <> transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "layers.2.mlp.fc2.bias torch.Size([768]) <> transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
      "layers.3.layer_norm1.weight torch.Size([768]) <> transformer.h.3.ln_1.weight torch.Size([768])\n",
      "layers.3.layer_norm1.bias torch.Size([768]) <> transformer.h.3.ln_1.bias torch.Size([768])\n",
      "layers.3.attention.qkv.weight torch.Size([2304, 768]) <> transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
      "layers.3.attention.qkv.bias torch.Size([2304]) <> transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
      "layers.3.attention.out.weight torch.Size([768, 768]) <> transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
      "layers.3.attention.out.bias torch.Size([768]) <> transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
      "layers.3.layer_norm2.weight torch.Size([768]) <> transformer.h.3.ln_2.weight torch.Size([768])\n",
      "layers.3.layer_norm2.bias torch.Size([768]) <> transformer.h.3.ln_2.bias torch.Size([768])\n",
      "layers.3.mlp.fc1.weight torch.Size([3072, 768]) <> transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "layers.3.mlp.fc1.bias torch.Size([3072]) <> transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
      "layers.3.mlp.fc2.weight torch.Size([768, 3072]) <> transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "layers.3.mlp.fc2.bias torch.Size([768]) <> transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
      "layers.4.layer_norm1.weight torch.Size([768]) <> transformer.h.4.ln_1.weight torch.Size([768])\n",
      "layers.4.layer_norm1.bias torch.Size([768]) <> transformer.h.4.ln_1.bias torch.Size([768])\n",
      "layers.4.attention.qkv.weight torch.Size([2304, 768]) <> transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
      "layers.4.attention.qkv.bias torch.Size([2304]) <> transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
      "layers.4.attention.out.weight torch.Size([768, 768]) <> transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
      "layers.4.attention.out.bias torch.Size([768]) <> transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
      "layers.4.layer_norm2.weight torch.Size([768]) <> transformer.h.4.ln_2.weight torch.Size([768])\n",
      "layers.4.layer_norm2.bias torch.Size([768]) <> transformer.h.4.ln_2.bias torch.Size([768])\n",
      "layers.4.mlp.fc1.weight torch.Size([3072, 768]) <> transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "layers.4.mlp.fc1.bias torch.Size([3072]) <> transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
      "layers.4.mlp.fc2.weight torch.Size([768, 3072]) <> transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "layers.4.mlp.fc2.bias torch.Size([768]) <> transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
      "layers.5.layer_norm1.weight torch.Size([768]) <> transformer.h.5.ln_1.weight torch.Size([768])\n",
      "layers.5.layer_norm1.bias torch.Size([768]) <> transformer.h.5.ln_1.bias torch.Size([768])\n",
      "layers.5.attention.qkv.weight torch.Size([2304, 768]) <> transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
      "layers.5.attention.qkv.bias torch.Size([2304]) <> transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
      "layers.5.attention.out.weight torch.Size([768, 768]) <> transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
      "layers.5.attention.out.bias torch.Size([768]) <> transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
      "layers.5.layer_norm2.weight torch.Size([768]) <> transformer.h.5.ln_2.weight torch.Size([768])\n",
      "layers.5.layer_norm2.bias torch.Size([768]) <> transformer.h.5.ln_2.bias torch.Size([768])\n",
      "layers.5.mlp.fc1.weight torch.Size([3072, 768]) <> transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "layers.5.mlp.fc1.bias torch.Size([3072]) <> transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
      "layers.5.mlp.fc2.weight torch.Size([768, 3072]) <> transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "layers.5.mlp.fc2.bias torch.Size([768]) <> transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
      "layers.6.layer_norm1.weight torch.Size([768]) <> transformer.h.6.ln_1.weight torch.Size([768])\n",
      "layers.6.layer_norm1.bias torch.Size([768]) <> transformer.h.6.ln_1.bias torch.Size([768])\n",
      "layers.6.attention.qkv.weight torch.Size([2304, 768]) <> transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
      "layers.6.attention.qkv.bias torch.Size([2304]) <> transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
      "layers.6.attention.out.weight torch.Size([768, 768]) <> transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
      "layers.6.attention.out.bias torch.Size([768]) <> transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
      "layers.6.layer_norm2.weight torch.Size([768]) <> transformer.h.6.ln_2.weight torch.Size([768])\n",
      "layers.6.layer_norm2.bias torch.Size([768]) <> transformer.h.6.ln_2.bias torch.Size([768])\n",
      "layers.6.mlp.fc1.weight torch.Size([3072, 768]) <> transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "layers.6.mlp.fc1.bias torch.Size([3072]) <> transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
      "layers.6.mlp.fc2.weight torch.Size([768, 3072]) <> transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "layers.6.mlp.fc2.bias torch.Size([768]) <> transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
      "layers.7.layer_norm1.weight torch.Size([768]) <> transformer.h.7.ln_1.weight torch.Size([768])\n",
      "layers.7.layer_norm1.bias torch.Size([768]) <> transformer.h.7.ln_1.bias torch.Size([768])\n",
      "layers.7.attention.qkv.weight torch.Size([2304, 768]) <> transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
      "layers.7.attention.qkv.bias torch.Size([2304]) <> transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
      "layers.7.attention.out.weight torch.Size([768, 768]) <> transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
      "layers.7.attention.out.bias torch.Size([768]) <> transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
      "layers.7.layer_norm2.weight torch.Size([768]) <> transformer.h.7.ln_2.weight torch.Size([768])\n",
      "layers.7.layer_norm2.bias torch.Size([768]) <> transformer.h.7.ln_2.bias torch.Size([768])\n",
      "layers.7.mlp.fc1.weight torch.Size([3072, 768]) <> transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "layers.7.mlp.fc1.bias torch.Size([3072]) <> transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
      "layers.7.mlp.fc2.weight torch.Size([768, 3072]) <> transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "layers.7.mlp.fc2.bias torch.Size([768]) <> transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
      "layers.8.layer_norm1.weight torch.Size([768]) <> transformer.h.8.ln_1.weight torch.Size([768])\n",
      "layers.8.layer_norm1.bias torch.Size([768]) <> transformer.h.8.ln_1.bias torch.Size([768])\n",
      "layers.8.attention.qkv.weight torch.Size([2304, 768]) <> transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
      "layers.8.attention.qkv.bias torch.Size([2304]) <> transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
      "layers.8.attention.out.weight torch.Size([768, 768]) <> transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
      "layers.8.attention.out.bias torch.Size([768]) <> transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
      "layers.8.layer_norm2.weight torch.Size([768]) <> transformer.h.8.ln_2.weight torch.Size([768])\n",
      "layers.8.layer_norm2.bias torch.Size([768]) <> transformer.h.8.ln_2.bias torch.Size([768])\n",
      "layers.8.mlp.fc1.weight torch.Size([3072, 768]) <> transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "layers.8.mlp.fc1.bias torch.Size([3072]) <> transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
      "layers.8.mlp.fc2.weight torch.Size([768, 3072]) <> transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "layers.8.mlp.fc2.bias torch.Size([768]) <> transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
      "layers.9.layer_norm1.weight torch.Size([768]) <> transformer.h.9.ln_1.weight torch.Size([768])\n",
      "layers.9.layer_norm1.bias torch.Size([768]) <> transformer.h.9.ln_1.bias torch.Size([768])\n",
      "layers.9.attention.qkv.weight torch.Size([2304, 768]) <> transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
      "layers.9.attention.qkv.bias torch.Size([2304]) <> transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
      "layers.9.attention.out.weight torch.Size([768, 768]) <> transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
      "layers.9.attention.out.bias torch.Size([768]) <> transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
      "layers.9.layer_norm2.weight torch.Size([768]) <> transformer.h.9.ln_2.weight torch.Size([768])\n",
      "layers.9.layer_norm2.bias torch.Size([768]) <> transformer.h.9.ln_2.bias torch.Size([768])\n",
      "layers.9.mlp.fc1.weight torch.Size([3072, 768]) <> transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "layers.9.mlp.fc1.bias torch.Size([3072]) <> transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
      "layers.9.mlp.fc2.weight torch.Size([768, 3072]) <> transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "layers.9.mlp.fc2.bias torch.Size([768]) <> transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
      "layers.10.layer_norm1.weight torch.Size([768]) <> transformer.h.10.ln_1.weight torch.Size([768])\n",
      "layers.10.layer_norm1.bias torch.Size([768]) <> transformer.h.10.ln_1.bias torch.Size([768])\n",
      "layers.10.attention.qkv.weight torch.Size([2304, 768]) <> transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
      "layers.10.attention.qkv.bias torch.Size([2304]) <> transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
      "layers.10.attention.out.weight torch.Size([768, 768]) <> transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
      "layers.10.attention.out.bias torch.Size([768]) <> transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
      "layers.10.layer_norm2.weight torch.Size([768]) <> transformer.h.10.ln_2.weight torch.Size([768])\n",
      "layers.10.layer_norm2.bias torch.Size([768]) <> transformer.h.10.ln_2.bias torch.Size([768])\n",
      "layers.10.mlp.fc1.weight torch.Size([3072, 768]) <> transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "layers.10.mlp.fc1.bias torch.Size([3072]) <> transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
      "layers.10.mlp.fc2.weight torch.Size([768, 3072]) <> transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "layers.10.mlp.fc2.bias torch.Size([768]) <> transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
      "layers.11.layer_norm1.weight torch.Size([768]) <> transformer.h.11.ln_1.weight torch.Size([768])\n",
      "layers.11.layer_norm1.bias torch.Size([768]) <> transformer.h.11.ln_1.bias torch.Size([768])\n",
      "layers.11.attention.qkv.weight torch.Size([2304, 768]) <> transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
      "layers.11.attention.qkv.bias torch.Size([2304]) <> transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
      "layers.11.attention.out.weight torch.Size([768, 768]) <> transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
      "layers.11.attention.out.bias torch.Size([768]) <> transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
      "layers.11.layer_norm2.weight torch.Size([768]) <> transformer.h.11.ln_2.weight torch.Size([768])\n",
      "layers.11.layer_norm2.bias torch.Size([768]) <> transformer.h.11.ln_2.bias torch.Size([768])\n",
      "layers.11.mlp.fc1.weight torch.Size([3072, 768]) <> transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "layers.11.mlp.fc1.bias torch.Size([3072]) <> transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
      "layers.11.mlp.fc2.weight torch.Size([768, 3072]) <> transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "layers.11.mlp.fc2.bias torch.Size([768]) <> transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
      "layer_norm.weight torch.Size([768]) <> transformer.ln_f.weight torch.Size([768])\n",
      "layer_norm.bias torch.Size([768]) <> transformer.ln_f.bias torch.Size([768])\n",
      "output_layer.weight torch.Size([50257, 768]) <> lm_head.weight torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "assert len(gpt2_model.state_dict().items()) == len(model.state_dict().items())\n",
    "\n",
    "for (key, value), (ref_key, ref_value) in zip(gpt2_model.state_dict().items(), model.state_dict().items()):\n",
    "    print(key, value.size(), \"<>\", ref_key, ref_value.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pre-trained weights\n",
    "\n",
    "We can see that number of parameters and dimensions match. In some cases, we notice that the dimensions are transposed as [Andrej mentioned in his youtube video](https://www.youtube.com/watch?v=l8pRSuU81PU&t=2593s) as well. \n",
    "\n",
    "> HF model uses `nn.Conv1d` for qkv projection while we use `nn.Linear`. Hence, we need to transpose some of these arguments. \n",
    "\n",
    "Let's write a method to load all the weights. For the follow up work, I have moved the existing code to a module gpt.py in the same directory. So, we will just add a classmethod to it. This is what the classmethod looks like\n",
    "\n",
    "```python\n",
    "    @classmethod\n",
    "    def from_pretrained(cls) -> 'GPT2':\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(\n",
    "            'gpt2', resume_download=None)\n",
    "        config = GPT2Config()\n",
    "        model = cls(config)\n",
    "        with torch.no_grad():\n",
    "            model.token_embedding.weight.copy_(model_hf.transformer.wte.weight)\n",
    "            model.positional_embedding.weight.copy_(\n",
    "                model_hf.transformer.wpe.weight)\n",
    "            for layer_idx in range(len(model.layers)):\n",
    "                model.layers[layer_idx].layer_norm1.weight.copy_(\n",
    "                    model_hf.transformer.h[layer_idx].ln_1.weight)\n",
    "                model.layers[layer_idx].layer_norm1.bias.copy_(\n",
    "                    model_hf.transformer.h[layer_idx].ln_1.bias)\n",
    "                # HF model uses Conv1d for qkv projection, we use linear.\n",
    "                # hence the transpose\n",
    "                model.layers[layer_idx].attention.qkv.weight.copy_(\n",
    "                    model_hf.transformer.h[layer_idx].attn.c_attn.weight.t())\n",
    "                model.layers[layer_idx].attention.qkv.bias.copy_(\n",
    "                    model_hf.transformer.h[layer_idx].attn.c_attn.bias)\n",
    "                model.layers[layer_idx].attention.out.weight.copy_(\n",
    "                    model_hf.transformer.h[layer_idx].attn.c_proj.weight.t())\n",
    "                model.layers[layer_idx].attention.out.bias.copy_(\n",
    "                    model_hf.transformer.h[layer_idx].attn.c_proj.bias)\n",
    "                model.layers[layer_idx].layer_norm2.weight.copy_(\n",
    "                    model_hf.transformer.h[layer_idx].ln_2.weight)\n",
    "                model.layers[layer_idx].layer_norm2.bias.copy_(\n",
    "                    model_hf.transformer.h[layer_idx].ln_2.bias)\n",
    "                model.layers[layer_idx].mlp.fc1.weight.copy_(\n",
    "                    model_hf.transformer.h[layer_idx].mlp.c_fc.weight.t())\n",
    "                model.layers[layer_idx].mlp.fc1.bias.copy_(\n",
    "                    model_hf.transformer.h[layer_idx].mlp.c_fc.bias)\n",
    "                model.layers[layer_idx].mlp.fc2.weight.copy_(\n",
    "                    model_hf.transformer.h[layer_idx].mlp.c_proj.weight.t())\n",
    "                model.layers[layer_idx].mlp.fc2.bias.copy_(\n",
    "                    model_hf.transformer.h[layer_idx].mlp.c_proj.bias)\n",
    "            model.layer_norm.weight.copy_(model_hf.transformer.ln_f.weight)\n",
    "            model.layer_norm.bias.copy_(model_hf.transformer.ln_f.bias)\n",
    "            model.output_layer.weight.copy_(model_hf.lm_head.weight)\n",
    "            # output_layer does not have a bias in gpt2\n",
    "            return model\n",
    "\n",
    "```\n",
    "\n",
    "Let's take this method for a spin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 50257])\n"
     ]
    }
   ],
   "source": [
    "from gpt import GPT2, GPT2Config\n",
    "\n",
    "gpt2_model = GPT2.from_pretrained()\n",
    "gpt2_out = gpt2_model(encoded_input['input_ids'])\n",
    "print(gpt2_out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should be able to test the 2 models for output now. We can feed both the models with a random input and check the final output at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "    hf_model = GPT2LMHeadModel.from_pretrained('gpt2', resume_download=None)\n",
    "    hf_model.eval()\n",
    "\n",
    "    gpt2_model = GPT2.from_pretrained()\n",
    "    gpt2_model.eval()\n",
    "\n",
    "    torch.random.manual_seed(42)\n",
    "    model_input = torch.randint(0, 50257, (1, 30))\n",
    "    gpt2_output = gpt2_model(model_input)\n",
    "    hf_output = hf_model(model_input)\n",
    "\n",
    "    assert torch.allclose(hf_output.logits, gpt2_output, atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome!! The outputs match within the allowed tolerance. Let's look at the outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -33.4287,  -33.3094,  -37.3657,  ...,  -41.1595,  -40.2192,\n",
       "           -34.0600],\n",
       "         [ -93.6944,  -94.3049,  -99.3645,  ..., -103.2246, -103.1845,\n",
       "           -97.7775],\n",
       "         [ -76.7458,  -77.8964,  -84.8150,  ...,  -86.7074,  -86.7198,\n",
       "           -80.1324],\n",
       "         ...,\n",
       "         [ -58.4519,  -57.2183,  -57.8318,  ...,  -63.7139,  -66.0336,\n",
       "           -59.0490],\n",
       "         [ -65.3561,  -66.7475,  -68.7738,  ...,  -72.2164,  -72.5056,\n",
       "           -66.6998],\n",
       "         [ -58.9639,  -59.1462,  -59.4149,  ...,  -62.6749,  -64.1926,\n",
       "           -59.5627]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -33.4287,  -33.3094,  -37.3657,  ...,  -41.1595,  -40.2192,\n",
       "           -34.0600],\n",
       "         [ -93.6943,  -94.3048,  -99.3645,  ..., -103.2245, -103.1845,\n",
       "           -97.7774],\n",
       "         [ -76.7457,  -77.8964,  -84.8149,  ...,  -86.7074,  -86.7197,\n",
       "           -80.1324],\n",
       "         ...,\n",
       "         [ -58.4519,  -57.2183,  -57.8318,  ...,  -63.7139,  -66.0336,\n",
       "           -59.0490],\n",
       "         [ -65.3561,  -66.7474,  -68.7738,  ...,  -72.2164,  -72.5056,\n",
       "           -66.6998],\n",
       "         [ -58.9640,  -59.1462,  -59.4150,  ...,  -62.6749,  -64.1926,\n",
       "           -59.5627]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Training Step\n",
    "\n",
    "In the previous section, we only tested the eval step. A number of dropouts do not run when we run the model in eval model. Let's switch both models to training mode to see whether they match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-79.6778, -80.5422, -86.3387,  ..., -89.9951, -87.5189, -83.0731]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "tensor([[[-79.6775, -80.5419, -86.3384,  ..., -89.9948, -87.5187, -83.0728]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hf_model = GPT2LMHeadModel.from_pretrained('gpt2', resume_download=None)\n",
    "hf_model.train()\n",
    "\n",
    "gpt2_model = GPT2.from_pretrained()\n",
    "gpt2_model.train()\n",
    "\n",
    "torch.random.manual_seed(42)\n",
    "model_input = torch.randint(0, 50257, (1, 1))\n",
    "\n",
    "# we need to set the manual_seed again to make sure dropouts get the\n",
    "# same ordered random numbers\n",
    "torch.random.manual_seed(42)\n",
    "gpt2_output = gpt2_model(model_input)\n",
    "\n",
    "# we need to set the manual_seed again to make sure dropouts get the\n",
    "# same ordered random numbers\n",
    "torch.random.manual_seed(42)\n",
    "hf_output = hf_model(model_input)\n",
    "\n",
    "print(hf_output.logits)\n",
    "print(gpt2_output)\n",
    "\n",
    "assert torch.allclose(hf_output.logits, gpt2_output, atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added both of the above checks for equivalnce as unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.14, pytest-7.4.0, pluggy-1.0.0 -- /Users/kapilsharma/.conda/envs/ml-projects/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/kapilsharma/oss/ml-projects/transformers\n",
      "plugins: anyio-4.2.0\n",
      "collected 2 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "test_gpt.py::test_model_correctness_eval \u001b[32mPASSED\u001b[0m\u001b[32m                          [ 50%]\u001b[0m\n",
      "test_gpt.py::test_model_correctness_train \u001b[32mPASSED\u001b[0m\u001b[32m                         [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 11.60s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -rx -v test_gpt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
