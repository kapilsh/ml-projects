{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from gpt import GPT2\n",
    "from transformers import pipeline, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded models\n"
     ]
    }
   ],
   "source": [
    "hf_model = GPT2LMHeadModel.from_pretrained('gpt2', resume_download=None).cuda()\n",
    "hf_model.eval()\n",
    "\n",
    "gpt2_model = GPT2.from_pretrained().cuda()\n",
    "gpt2_model.eval()\n",
    "\n",
    "print(\"Loaded models\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every power of 2 token size up to 1024, let's compare the output of the two models\n",
    "\n",
    "def compare_models(hf_model, gpt2_model, tokenizer, max_token_size=1024):\n",
    "    for token_size in [2 ** i for i in range(int(math.log2(max_token_size)) + 1)]:\n",
    "        input_ids = torch.randint(0, tokenizer.vocab_size, (1, token_size)).cuda()\n",
    "        print(f\"Token size: {token_size}, input_ids size {input_ids.size()}\")\n",
    "        with torch.no_grad():\n",
    "            hf_output = hf_model(input_ids)\n",
    "            gpt2_output = gpt2_model(input_ids)\n",
    "        assert torch.allclose(hf_output.logits, gpt2_output, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ksharma/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token size: 1, input_ids size torch.Size([1, 1])\n",
      "Token size: 2, input_ids size torch.Size([1, 2])\n",
      "Token size: 4, input_ids size torch.Size([1, 4])\n",
      "Token size: 8, input_ids size torch.Size([1, 8])\n",
      "Token size: 16, input_ids size torch.Size([1, 16])\n",
      "Token size: 32, input_ids size torch.Size([1, 32])\n",
      "Token size: 64, input_ids size torch.Size([1, 64])\n",
      "Token size: 128, input_ids size torch.Size([1, 128])\n",
      "Token size: 256, input_ids size torch.Size([1, 256])\n",
      "Token size: 512, input_ids size torch.Size([1, 512])\n",
      "Token size: 1024, input_ids size torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "compare_models(hf_model, gpt2_model, GPT2Tokenizer.from_pretrained('gpt2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sample inputs for 1024 tokens and batch size 1\n",
    "input_ids_list = [torch.randint(0, 50256, (1, 1024)).cuda() for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_model = gpt2_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-07-27 07:48:07 127521:127521 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-07-27 07:48:07 127521:127521 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-07-27 07:48:07 127521:127521 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"./performance_trace\", flush_secs=30)\n",
    "\n",
    "prof = torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ],\n",
    "\n",
    "    # In this example with wait=1, warmup=1, active=2, repeat=1,\n",
    "    # profiler will skip the first step/iteration,\n",
    "    # start warming up on the second, record\n",
    "    # the third and the forth iterations,\n",
    "    # after which the trace will become available\n",
    "    # and on_trace_ready (when set) is called;\n",
    "    # the cycle repeats starting with the next step\n",
    "\n",
    "    schedule=torch.profiler.schedule(\n",
    "        wait=1,\n",
    "        warmup=1,\n",
    "        active=3,\n",
    "        repeat=1),\n",
    "    on_trace_ready=torch.profiler.tensorboard_trace_handler(\"./performance_trace\"),\n",
    "    # record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    with_stack=True\n",
    "    # used when outputting for tensorboard\n",
    ")\n",
    "prof.start()\n",
    "for input_ids in input_ids_list:\n",
    "    with torch.no_grad():\n",
    "        _ = gpt2_model(input_ids)\n",
    "        prof.step()\n",
    "prof.stop()\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
