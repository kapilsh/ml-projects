{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 30 14:16:04 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off | 00000000:01:00.0  On |                  Off |\n",
      "| 30%   44C    P5              33W / 450W |    430MiB / 24564MiB |     24%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1949      G   /usr/lib/xorg/Xorg                          164MiB |\n",
      "|    0   N/A  N/A      2170      G   /usr/bin/gnome-shell                         53MiB |\n",
      "|    0   N/A  N/A      3823      G   ...seed-version=20240725-094838.558000       52MiB |\n",
      "|    0   N/A  N/A      7258      G   ...erProcess --variations-seed-version      144MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ksharma/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import TextFileDataset\n",
    "from gpt import GPT2\n",
    "from transformers import pipeline, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded models\n"
     ]
    }
   ],
   "source": [
    "hf_model = GPT2LMHeadModel.from_pretrained('gpt2', resume_download=None).cuda()\n",
    "hf_model.eval()\n",
    "\n",
    "gpt2_model = GPT2.from_pretrained().cuda()\n",
    "gpt2_model.eval()\n",
    "\n",
    "print(\"Loaded models\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every power of 2 token size up to 1024, let's compare the output of the two models\n",
    "\n",
    "def compare_models(hf_model, gpt2_model, tokenizer, max_token_size=1024):\n",
    "    for token_size in [2 ** i for i in range(int(math.log2(max_token_size)) + 1)]:\n",
    "        input_ids = torch.randint(0, tokenizer.vocab_size, (1, token_size)).cuda()\n",
    "        print(f\"Token size: {token_size}, input_ids size {input_ids.size()}\")\n",
    "        with torch.no_grad():\n",
    "            hf_output = hf_model(input_ids)\n",
    "            gpt2_output = gpt2_model(input_ids)\n",
    "        assert torch.allclose(hf_output.logits, gpt2_output, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token size: 1, input_ids size torch.Size([1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ksharma/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token size: 2, input_ids size torch.Size([1, 2])\n",
      "Token size: 4, input_ids size torch.Size([1, 4])\n",
      "Token size: 8, input_ids size torch.Size([1, 8])\n",
      "Token size: 16, input_ids size torch.Size([1, 16])\n",
      "Token size: 32, input_ids size torch.Size([1, 32])\n",
      "Token size: 64, input_ids size torch.Size([1, 64])\n",
      "Token size: 128, input_ids size torch.Size([1, 128])\n",
      "Token size: 256, input_ids size torch.Size([1, 256])\n",
      "Token size: 512, input_ids size torch.Size([1, 512])\n",
      "Token size: 1024, input_ids size torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "compare_models(hf_model, gpt2_model, GPT2Tokenizer.from_pretrained('gpt2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): GPT2(\n",
       "    (token_embedding): Embedding(50257, 768)\n",
       "    (positional_embedding): Embedding(1024, 768)\n",
       "    (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x GPT2Layer(\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): CausalMultiHeadAttention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (residual_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (activation): GELU(approximate='tanh')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (output_layer): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_model = gpt2_model.cuda()\n",
    "gpt2_model_compiled = torch.compile(gpt2_model.cuda(), mode=\"max-autotune\")\n",
    "gpt2_model_compiled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (141170 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from dataset import TextFileDataset\n",
    "dataset = TextFileDataset(\"data/1984.txt\", sequence_length=1024)\n",
    "dl = iter(DataLoader(dataset, batch_size=8, shuffle=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens, next_token = next(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1024])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens processed per second: 35621\n",
      "Tokens processed per second: 27847\n",
      "Tokens processed per second: 36239\n",
      "Tokens processed per second: 33874\n",
      "Tokens processed per second: 32038\n",
      "Tokens processed per second: 30608\n",
      "Tokens processed per second: 28986\n",
      "Tokens processed per second: 27800\n",
      "Tokens processed per second: 26262\n",
      "Tokens processed per second: 25368\n",
      "Tokens processed per second: 24330\n",
      "Tokens processed per second: 23415\n",
      "Tokens processed per second: 22534\n",
      "Tokens processed per second: 21721\n",
      "Tokens processed per second: 20923\n",
      "Tokens processed per second: 20150\n",
      "Tokens processed per second: 19465\n",
      "Tokens processed per second: 18924\n",
      "Tokens processed per second: 18316\n",
      "Tokens processed per second: 17814\n",
      "Tokens processed per second: 17088\n",
      "Tokens processed per second: 16792\n",
      "Tokens processed per second: 15795\n",
      "Tokens processed per second: 15798\n",
      "Tokens processed per second: 15237\n",
      "Tokens processed per second: 14894\n",
      "Tokens processed per second: 14581\n",
      "Tokens processed per second: 14279\n",
      "Tokens processed per second: 13996\n",
      "Tokens processed per second: 13637\n",
      "Tokens processed per second: 13130\n",
      "Tokens processed per second: 13071\n",
      "Tokens processed per second: 12758\n",
      "Tokens processed per second: 12499\n",
      "Tokens processed per second: 12204\n",
      "Tokens processed per second: 12050\n",
      "Tokens processed per second: 11708\n",
      "Tokens processed per second: 11525\n",
      "Tokens processed per second: 11288\n",
      "Tokens processed per second: 10923\n",
      "Tokens processed per second: 9792\n",
      "Tokens processed per second: 10630\n",
      "Tokens processed per second: 10481\n",
      "Tokens processed per second: 10287\n",
      "Tokens processed per second: 10161\n",
      "Tokens processed per second: 9976\n",
      "Tokens processed per second: 9883\n",
      "Tokens processed per second: 9742\n",
      "Tokens processed per second: 9564\n",
      "Tokens processed per second: 9434\n",
      "Tokens processed per second: 9302\n",
      "Tokens processed per second: 9150\n",
      "Tokens processed per second: 8980\n",
      "Tokens processed per second: 8872\n",
      "Tokens processed per second: 8752\n",
      "Tokens processed per second: 8652\n",
      "Tokens processed per second: 8413\n",
      "Tokens processed per second: 8353\n",
      "Tokens processed per second: 8210\n",
      "Tokens processed per second: 8136\n",
      "Tokens processed per second: 8014\n",
      "Tokens processed per second: 7907\n",
      "Tokens processed per second: 7821\n",
      "Tokens processed per second: 7741\n",
      "Tokens processed per second: 7596\n",
      "Tokens processed per second: 7484\n",
      "Tokens processed per second: 7408\n",
      "Tokens processed per second: 7303\n",
      "Tokens processed per second: 7222\n",
      "Tokens processed per second: 7105\n",
      "Tokens processed per second: 7016\n",
      "Tokens processed per second: 6985\n",
      "Tokens processed per second: 6807\n",
      "Tokens processed per second: 6808\n",
      "Tokens processed per second: 6750\n",
      "Tokens processed per second: 6671\n",
      "Tokens processed per second: 6521\n",
      "Tokens processed per second: 6502\n",
      "Tokens processed per second: 6471\n",
      "Tokens processed per second: 6409\n",
      "Tokens processed per second: 6376\n",
      "Tokens processed per second: 6320\n",
      "Tokens processed per second: 6223\n",
      "Tokens processed per second: 6055\n",
      "Tokens processed per second: 6055\n",
      "Tokens processed per second: 5956\n",
      "Tokens processed per second: 5826\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      3\u001b[0m input_tokens, next_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dl)\n\u001b[0;32m----> 4\u001b[0m _ \u001b[38;5;241m=\u001b[39m gpt2_model_compiled(input_tokens\u001b[38;5;241m.\u001b[39mcuda())\n\u001b[1;32m      5\u001b[0m tokens_processed \u001b[38;5;241m=\u001b[39m input_tokens\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m input_tokens\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m/\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m time_start)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokens processed per second: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokens_processed\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.0f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:451\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/git/ml-projects/transformers/gpt.py:41\u001b[0m, in \u001b[0;36mGPT2.forward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# gpt2 does not use a bias in the output layer\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39membedding_dimension,\n\u001b[1;32m     39\u001b[0m                                   config\u001b[38;5;241m.\u001b[39mvocab_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     42\u001b[0m     _, current_sequence_length \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m     43\u001b[0m     positions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_ids[:, :current_sequence_length]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:451\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:36\u001b[0m, in \u001b[0;36mwrap_inline.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:917\u001b[0m, in \u001b[0;36maot_module_simplified.<locals>.forward\u001b[0;34m(*runtime_args)\u001b[0m\n\u001b[1;32m    915\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(params_flat)\n\u001b[1;32m    916\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(runtime_args)\n\u001b[0;32m--> 917\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn(full_args)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py:89\u001b[0m, in \u001b[0;36mmake_boxed_func.<locals>.g\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mg\u001b[39m(args):\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:88\u001b[0m, in \u001b[0;36mcreate_runtime_wrapper.<locals>.runtime_wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     86\u001b[0m             args_[idx] \u001b[38;5;241m=\u001b[39m args_[idx]\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39m_force_original_view_tracking(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 88\u001b[0m         all_outs \u001b[38;5;241m=\u001b[39m call_func_at_runtime_with_args(\n\u001b[1;32m     89\u001b[0m             compiled_fn,\n\u001b[1;32m     90\u001b[0m             args_,\n\u001b[1;32m     91\u001b[0m             disable_amp\u001b[38;5;241m=\u001b[39mdisable_amp,\n\u001b[1;32m     92\u001b[0m         )\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# When we have an inference graph, we run with torch.no_grad.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# It's possible to get an inference graph with inputs that require grad,\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# in which case we want to make sure autograd is disabled\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;66;03m# (since e.g., inductor will generate aten.addmm.out calls which autograd will complain on)\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_grad_enabled():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py:113\u001b[0m, in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 113\u001b[0m         out \u001b[38;5;241m=\u001b[39m normalize_as_list(f(args))\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m    117\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    118\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt take boxed arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py:89\u001b[0m, in \u001b[0;36mmake_boxed_func.<locals>.g\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mg\u001b[39m(args):\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/function.py:598\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    606\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:505\u001b[0m, in \u001b[0;36maot_dispatch_autograd.<locals>.CompiledFunction.forward\u001b[0;34m(ctx, *deduped_flat_tensor_args)\u001b[0m\n\u001b[1;32m    498\u001b[0m         args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39margs, seed, offset)\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;66;03m# There is a pretty complicated calling convention around what the compiled fw returns.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;66;03m# The full list of outputs and their relative order is:\u001b[39;00m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;66;03m# (*tokens, *mutated_inputs, *fw_outs, *fw_intermediate_bases, *saved_tensors, *saved_symints)\u001b[39;00m\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;66;03m# - Note that in the synthetic bases case, mutated_inputs will correspond to an updated version\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;66;03m#   of the original view, and not the synthetic base\u001b[39;00m\n\u001b[0;32m--> 505\u001b[0m     fw_outs \u001b[38;5;241m=\u001b[39m call_func_at_runtime_with_args(\n\u001b[1;32m    506\u001b[0m         CompiledFunction\u001b[38;5;241m.\u001b[39mcompiled_fw,\n\u001b[1;32m    507\u001b[0m         args,\n\u001b[1;32m    508\u001b[0m         disable_amp\u001b[38;5;241m=\u001b[39mdisable_amp,\n\u001b[1;32m    509\u001b[0m     )\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m fakified_out\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py:113\u001b[0m, in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 113\u001b[0m         out \u001b[38;5;241m=\u001b[39m normalize_as_list(f(args))\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m    117\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    118\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt take boxed arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_inductor/codecache.py:906\u001b[0m, in \u001b[0;36mCompiledFxGraph.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: List[Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 906\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_current_callable()(inputs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:838\u001b[0m, in \u001b[0;36mcudagraphify.<locals>.run\u001b[0;34m(new_inputs)\u001b[0m\n\u001b[1;32m    836\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_utils\u001b[38;5;241m.\u001b[39mpreserve_rng_state():\n\u001b[1;32m    837\u001b[0m         compiled_fn \u001b[38;5;241m=\u001b[39m cudagraphify_fn(model, new_inputs, static_input_idxs)\n\u001b[0;32m--> 838\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn(new_inputs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py:370\u001b[0m, in \u001b[0;36mcudagraphify_impl.<locals>.deferred_cudagraphify\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m    368\u001b[0m fn \u001b[38;5;241m=\u001b[39m fn_cache\u001b[38;5;241m.\u001b[39mget(int_key)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(inputs)\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m int_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    373\u001b[0m     log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecording cudagraph tree for graph without symints\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:784\u001b[0m, in \u001b[0;36malign_inputs_from_check_idxs.<locals>.run\u001b[0;34m(new_inputs)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(new_inputs):\n\u001b[1;32m    783\u001b[0m     copy_misaligned_inputs(new_inputs, inputs_to_check)\n\u001b[0;32m--> 784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model(new_inputs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py:1757\u001b[0m, in \u001b[0;36mCUDAGraphTreeManager.run\u001b[0;34m(self, new_inputs, function_id)\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, new_inputs: List[Tensor], function_id: FunctionID):\n\u001b[1;32m   1756\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning CUDAGraph after shutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1757\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(new_inputs, function_id)\n\u001b[1;32m   1759\u001b[0m     \u001b[38;5;66;03m# The forwards are only pending following invocation, not before\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid_to_mode[function_id]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py:1831\u001b[0m, in \u001b[0;36mCUDAGraphTreeManager._run\u001b[0;34m(self, new_inputs, function_id)\u001b[0m\n\u001b[1;32m   1828\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_checkpoint_execution_state_in_allocator()\n\u001b[1;32m   1830\u001b[0m \u001b[38;5;66;03m# now, we are in a recording state !\u001b[39;00m\n\u001b[0;32m-> 1831\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord_function(new_inputs, function_id)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py:1862\u001b[0m, in \u001b[0;36mCUDAGraphTreeManager.record_function\u001b[0;34m(self, new_inputs, function_id)\u001b[0m\n\u001b[1;32m   1856\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m   1857\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecording function \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m of graph recording id \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1858\u001b[0m     function_id\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m   1859\u001b[0m     graph_id\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m   1860\u001b[0m )\n\u001b[1;32m   1861\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[0;32m-> 1862\u001b[0m node \u001b[38;5;241m=\u001b[39m CUDAGraphNode(\n\u001b[1;32m   1863\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mids_to_funcs[function_id],\n\u001b[1;32m   1864\u001b[0m     graph_id,\n\u001b[1;32m   1865\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_node,\n\u001b[1;32m   1866\u001b[0m     new_inputs,\n\u001b[1;32m   1867\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcuda_graphs_thread_pool,\n\u001b[1;32m   1868\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_index,\n\u001b[1;32m   1869\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mids_to_stack_traces[function_id],\n\u001b[1;32m   1870\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream,\n\u001b[1;32m   1871\u001b[0m )\n\u001b[1;32m   1872\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1873\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroots[function_id]\u001b[38;5;241m.\u001b[39mappend(node)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py:902\u001b[0m, in \u001b[0;36mCUDAGraphNode.__init__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatic_output_tensors: OutputList[Optional[Tensor]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    899\u001b[0m \u001b[38;5;66;03m# Cleared after recording\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecording_outputs: Optional[\n\u001b[1;32m    901\u001b[0m     OutputList[Union[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]]\n\u001b[0;32m--> 902\u001b[0m ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record(wrapped_function\u001b[38;5;241m.\u001b[39mmodel, recording_inputs)\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs_metadata: OutputList[Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    905\u001b[0m \u001b[38;5;66;03m# As with inputs, we do not want to keep the outputs permanently alive because that would prevent\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;66;03m# their memory being reclaimed in subsequent cuda graph recordings. We record the tensor metadata\u001b[39;00m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;66;03m# needed to reconstruct instead.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py:1110\u001b[0m, in \u001b[0;36mCUDAGraphNode._record\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(static_outputs, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m   1108\u001b[0m     static_outputs \u001b[38;5;241m=\u001b[39m (static_outputs,)\n\u001b[0;32m-> 1110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_first_outputs(static_outputs, static_input_persistent_storage_ptrs)\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m static_outputs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py:1160\u001b[0m, in \u001b[0;36mCUDAGraphNode._add_first_outputs\u001b[0;34m(self, outputs, static_input_persistent_storage_ptrs)\u001b[0m\n\u001b[1;32m   1157\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatic_output_tensors[i] \u001b[38;5;241m=\u001b[39m o\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m-> 1160\u001b[0m path_ref \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_alias_of_live_recorded_tensor(o)\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path_ref \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mark_prior_graph_output_as_aliased(path_ref)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py:1304\u001b[0m, in \u001b[0;36mCUDAGraphNode._is_alias_of_live_recorded_tensor\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m depth, output_refs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_weakrefs):\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output_index, storage_ref \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(output_refs):\n\u001b[0;32m-> 1304\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (storage_and_ptr \u001b[38;5;241m:=\u001b[39m maybe_deref(storage_ref)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1305\u001b[0m             storage, ptr \u001b[38;5;241m=\u001b[39m storage_and_ptr\n\u001b[1;32m   1306\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m ptr \u001b[38;5;241m==\u001b[39m t\u001b[38;5;241m.\u001b[39muntyped_storage()\u001b[38;5;241m.\u001b[39mdata_ptr():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py:498\u001b[0m, in \u001b[0;36mmaybe_deref\u001b[0;34m(weak_ref)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weak_ref \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 498\u001b[0m r \u001b[38;5;241m=\u001b[39m weak_ref()\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py:458\u001b[0m, in \u001b[0;36mStorageWeakRefWrapper.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[StorageWeakRefPointer]:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpired():\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mref\u001b[38;5;241m.\u001b[39mcdata\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py:479\u001b[0m, in \u001b[0;36mStorageWeakRefWrapper.expired\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;66;03m# if extra_ref_check is not None we expect an additional reference\u001b[39;00m\n\u001b[0;32m--> 479\u001b[0m stor_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_storage_Use_Count(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mref\u001b[38;5;241m.\u001b[39mcdata)\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (stor_count \u001b[38;5;241m-\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_ref_check \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    time_start = time.time()\n",
    "    input_tokens, next_token = next(dl)\n",
    "    _ = gpt2_model_compiled(input_tokens.cuda())\n",
    "    tokens_processed = input_tokens.size(1) * input_tokens.size(0) / (time.time() - time_start)\n",
    "    print(f\"Tokens processed per second: {tokens_processed:.0f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sample inputs for 1024 tokens and batch size 1\n",
    "input_ids_list = [torch.randint(0, 50256, (16, 1024)).cuda() for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_profiler() -> torch.profiler.profile:\n",
    "    return torch.profiler.profile(\n",
    "        activities=[\n",
    "            torch.profiler.ProfilerActivity.CPU,\n",
    "            torch.profiler.ProfilerActivity.CUDA,\n",
    "        ],\n",
    "\n",
    "        # In this example with wait=1, warmup=1, active=2, repeat=1,\n",
    "        # profiler will skip the first step/iteration,\n",
    "        # start warming up on the second, record\n",
    "        # the third and the forth iterations,\n",
    "        # after which the trace will become available\n",
    "        # and on_trace_ready (when set) is called;\n",
    "        # the cycle repeats starting with the next step\n",
    "\n",
    "        schedule=torch.profiler.schedule(\n",
    "            wait=1,\n",
    "            warmup=1,\n",
    "            active=3,\n",
    "            repeat=1),\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(\"./performance_trace\"),\n",
    "        # record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True\n",
    "        # used when outputting for tensorboard\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile Eager Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"./performance_trace\", flush_secs=30)\n",
    "\n",
    "prof = get_profiler()\n",
    "prof.start()\n",
    "for input_ids in input_ids_list:\n",
    "    with torch.no_grad():\n",
    "        _ = gpt2_model(input_ids)\n",
    "        prof.step()\n",
    "prof.stop()\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile the compiled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir=\"./performance_trace\", flush_secs=30)\n",
    "\n",
    "prof = get_profiler()\n",
    "prof.start()\n",
    "for input_ids in input_ids_list:\n",
    "    with torch.no_grad():\n",
    "        _ = gpt2_model_compiled(input_ids)\n",
    "        prof.step()\n",
    "prof.stop()\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
