{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "injured-rouge",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "utility-orleans",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "infinite-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "martial-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./data/lotr.txt\"\n",
    "# file_path = \"/data/text8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "qualified-prediction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three Rings for the Elven-kings under the sky,\n",
      "               Seven for the Dwarf-lords in their halls of stone,\n",
      "            Nine for Mortal Men doomed to die,\n",
      "              One for the Dark Lord on h\n"
     ]
    }
   ],
   "source": [
    "with open(file_path) as f:\n",
    "    text = f.read()\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-plenty",
   "metadata": {},
   "source": [
    "# Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "integral-graham",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, remove_frequency):\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "    punctuations = {\n",
    "        '.': ' <PERIOD> ',\n",
    "        ',': ' <COMMA> ',\n",
    "        '\"': ' <QUOTATION_MARK> ',\n",
    "        '\\'': ' <QUOTATION_MARK> ',\n",
    "        ';': ' <SEMICOLON> ',\n",
    "        '!': ' <EXCLAMATION_MARK> ',\n",
    "        '?': ' <QUESTION_MARK> ',\n",
    "        '(': ' <LEFT_PAREN> ',\n",
    "        ')': ' <RIGHT_PAREN> ',\n",
    "        '--': ' <HYPHENS> ',\n",
    "        '?': ' <QUESTION_MARK> ',\n",
    "        ':': ' <COLON> '\n",
    "    }\n",
    "\n",
    "    # Replace punctuation markers\n",
    "    text = text.lower()\n",
    "    for key, value in punctuations.items():\n",
    "        text = text.replace(key, value)\n",
    "    words = text.split()\n",
    "\n",
    "    stop_words_set = set(stopwords.words())\n",
    "    for punctuation_token in punctuations.values():\n",
    "        stop_words_set.add(punctuation_token.strip())\n",
    "\n",
    "    # filter out in-frequent and stop words\n",
    "    word_counts = Counter(words)\n",
    "    filtered = [word for word in words if (word_counts[word] > remove_frequency and word not in stop_words_set)]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "otherwise-linux",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ksharma/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "filter_frequency = 10\n",
    "words = preprocess(text, filter_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "hydraulic-sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = Counter(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-haiti",
   "metadata": {},
   "source": [
    "# Top Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "functional-windows",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = list(word_counts.keys())\n",
    "unique_words.sort(key=lambda x: -word_counts[x]) # descending order\n",
    "word_tokens = {word: idx for idx, word in enumerate(unique_words)}\n",
    "total_counts = sum(word_counts.values())\n",
    "word_frequencies = {x: y / total_counts for x, y in word_counts.items()}\n",
    "top_20 = unique_words[:20]\n",
    "top_20_frequency = [word_frequencies[x] for x in top_20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "musical-scholarship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "ax.bar(x=top_20, height=top_20_frequency)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-bankruptcy",
   "metadata": {},
   "source": [
    "# Mini-batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "finnish-bridal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_context(idx, words, window):\n",
    "    half = window // 2\n",
    "    left = max(0, idx - half)\n",
    "    right = min(left + window, len(words))\n",
    "    if right == len(words):\n",
    "        left = right - window\n",
    "    surrounding_words = words[left:right]\n",
    "    context_tokens = [word_tokens[x] for x in surrounding_words if x != words[idx]]\n",
    "    return context_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "mobile-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(words, batch_size, window):\n",
    "    batch_count = len(words) // batch_size\n",
    "    words = words[:(batch_size * batch_count)]\n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx:idx + batch_size]\n",
    "        for i, word in enumerate(batch):\n",
    "            word_token = word_tokens[word]\n",
    "            context_tokens = generate_word_context(i, words, window)\n",
    "            x.extend([word_token] * len(context_tokens))\n",
    "            y.extend(context_tokens)\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "spare-duncan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([160,\n",
       "  160,\n",
       "  160,\n",
       "  700,\n",
       "  700,\n",
       "  700,\n",
       "  200,\n",
       "  200,\n",
       "  200,\n",
       "  625,\n",
       "  625,\n",
       "  625,\n",
       "  1149,\n",
       "  1149,\n",
       "  1149],\n",
       " [700, 200, 625, 160, 200, 625, 160, 700, 625, 700, 200, 1149, 200, 625, 108])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = generate_batches(words, 5, 4)\n",
    "x, y = next(batches)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "editorial-brazil",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, n_vocab, n_embed):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(n_vocab, n_embed)\n",
    "        self.output = nn.Linear(n_embed, n_vocab)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        scores = self.output(x)\n",
    "        log_likelihood = self.log_softmax(scores)\n",
    "        # Alternatively, we can use the scores directly and apply Cross-entropy loss\n",
    "        return log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "defined-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = torch.LongTensor(x), torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "piano-seeking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 2735])\n"
     ]
    }
   ],
   "source": [
    "skip_gram_model = SkipGram(len(word_tokens), 20)\n",
    "outputs = skip_gram_model(inputs)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "understanding-wisdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "obvious-multiple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.9918, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "attractive-warehouse",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-27 22:12:35.996 | INFO     | __main__:<module>:13 - Started epoch 1\n",
      "2021-04-27 22:12:36.499 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 100]: Loss Value: 4.845653533935547\n",
      "2021-04-27 22:12:36.970 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 200]: Loss Value: 4.126715183258057\n",
      "2021-04-27 22:12:37.435 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 300]: Loss Value: 4.03758430480957\n",
      "2021-04-27 22:12:37.942 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 400]: Loss Value: 4.009410381317139\n",
      "2021-04-27 22:12:38.415 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 500]: Loss Value: 3.491919994354248\n",
      "2021-04-27 22:12:38.868 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 600]: Loss Value: 3.4958407878875732\n",
      "2021-04-27 22:12:39.318 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 700]: Loss Value: 3.3863954544067383\n",
      "2021-04-27 22:12:39.774 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 800]: Loss Value: 3.7186851501464844\n",
      "2021-04-27 22:12:40.228 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 900]: Loss Value: 3.372166633605957\n",
      "2021-04-27 22:12:40.680 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 1000]: Loss Value: 3.3057730197906494\n",
      "2021-04-27 22:12:41.131 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 1100]: Loss Value: 3.4033424854278564\n",
      "2021-04-27 22:12:41.583 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 1200]: Loss Value: 3.322899103164673\n",
      "2021-04-27 22:12:42.034 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 1300]: Loss Value: 3.389634609222412\n",
      "2021-04-27 22:12:42.484 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 1400]: Loss Value: 3.3155999183654785\n",
      "2021-04-27 22:12:42.941 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 1500]: Loss Value: 3.291747808456421\n",
      "2021-04-27 22:12:43.399 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 1600]: Loss Value: 3.2024402618408203\n",
      "2021-04-27 22:12:43.853 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 1700]: Loss Value: 3.373445510864258\n",
      "2021-04-27 22:12:44.315 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 1800]: Loss Value: 3.334277868270874\n",
      "2021-04-27 22:12:44.771 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 1900]: Loss Value: 3.3312127590179443\n",
      "2021-04-27 22:12:45.232 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 2000]: Loss Value: 3.2868776321411133\n",
      "2021-04-27 22:12:45.698 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 2100]: Loss Value: 3.4028666019439697\n",
      "2021-04-27 22:12:46.155 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 2200]: Loss Value: 3.300078868865967\n",
      "2021-04-27 22:12:46.612 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 2300]: Loss Value: 3.283865213394165\n",
      "2021-04-27 22:12:47.067 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 2400]: Loss Value: 3.3431191444396973\n",
      "2021-04-27 22:12:47.529 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 2500]: Loss Value: 3.2192835807800293\n",
      "2021-04-27 22:12:48.014 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 2600]: Loss Value: 3.2428483963012695\n",
      "2021-04-27 22:12:48.480 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 2700]: Loss Value: 3.20029616355896\n",
      "2021-04-27 22:12:48.937 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 2800]: Loss Value: 3.353198289871216\n",
      "2021-04-27 22:12:49.412 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 2900]: Loss Value: 3.325765371322632\n",
      "2021-04-27 22:12:49.874 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 3000]: Loss Value: 3.1769425868988037\n",
      "2021-04-27 22:12:50.380 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 3100]: Loss Value: 3.2471976280212402\n",
      "2021-04-27 22:12:50.879 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 3200]: Loss Value: 3.26149320602417\n",
      "2021-04-27 22:12:51.340 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 3300]: Loss Value: 3.3331565856933594\n",
      "2021-04-27 22:12:51.806 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 3400]: Loss Value: 3.192844867706299\n",
      "2021-04-27 22:12:52.264 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 3500]: Loss Value: 3.2808268070220947\n",
      "2021-04-27 22:12:52.722 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 3600]: Loss Value: 3.286771297454834\n",
      "2021-04-27 22:12:53.187 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 3700]: Loss Value: 3.31852650642395\n",
      "2021-04-27 22:12:53.651 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 3800]: Loss Value: 3.2417640686035156\n",
      "2021-04-27 22:12:54.112 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 3900]: Loss Value: 3.329531669616699\n",
      "2021-04-27 22:12:54.579 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 4000]: Loss Value: 3.3867838382720947\n",
      "2021-04-27 22:12:55.042 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 4100]: Loss Value: 3.3051140308380127\n",
      "2021-04-27 22:12:55.509 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 4200]: Loss Value: 3.3305373191833496\n",
      "2021-04-27 22:12:55.966 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 4300]: Loss Value: 3.3036680221557617\n",
      "2021-04-27 22:12:56.430 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 4400]: Loss Value: 3.2821052074432373\n",
      "2021-04-27 22:12:56.889 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 4500]: Loss Value: 3.259982109069824\n",
      "2021-04-27 22:12:57.347 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 4600]: Loss Value: 3.291121482849121\n",
      "2021-04-27 22:12:57.807 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 4700]: Loss Value: 3.325718641281128\n",
      "2021-04-27 22:12:58.268 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 4800]: Loss Value: 3.3652377128601074\n",
      "2021-04-27 22:12:58.733 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 4900]: Loss Value: 3.262815475463867\n",
      "2021-04-27 22:12:59.197 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 5000]: Loss Value: 3.3332338333129883\n",
      "2021-04-27 22:12:59.669 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 5100]: Loss Value: 3.2675130367279053\n",
      "2021-04-27 22:13:00.147 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 5200]: Loss Value: 3.3060576915740967\n",
      "2021-04-27 22:13:00.627 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 5300]: Loss Value: 3.3514440059661865\n",
      "2021-04-27 22:13:01.105 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 5400]: Loss Value: 3.2771294116973877\n",
      "2021-04-27 22:13:01.578 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 5500]: Loss Value: 3.285618305206299\n",
      "2021-04-27 22:13:02.051 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 5600]: Loss Value: 3.277904510498047\n",
      "2021-04-27 22:13:02.522 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 5700]: Loss Value: 3.3674378395080566\n",
      "2021-04-27 22:13:02.993 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 5800]: Loss Value: 3.1989235877990723\n",
      "2021-04-27 22:13:03.468 | INFO     | __main__:<module>:23 - [Epoch 1] [Step 5900]: Loss Value: 3.3299715518951416\n",
      "2021-04-27 22:13:03.770 | INFO     | __main__:<module>:13 - Started epoch 2\n",
      "2021-04-27 22:13:03.955 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 6000]: Loss Value: 3.293043375015259\n",
      "2021-04-27 22:13:04.437 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 6100]: Loss Value: 3.306269407272339\n",
      "2021-04-27 22:13:04.913 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 6200]: Loss Value: 3.2540507316589355\n",
      "2021-04-27 22:13:05.389 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 6300]: Loss Value: 3.3520073890686035\n",
      "2021-04-27 22:13:05.922 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 6400]: Loss Value: 3.2997887134552\n",
      "2021-04-27 22:13:06.412 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 6500]: Loss Value: 3.2176668643951416\n",
      "2021-04-27 22:13:06.901 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 6600]: Loss Value: 3.2401347160339355\n",
      "2021-04-27 22:13:07.379 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 6700]: Loss Value: 3.406282424926758\n",
      "2021-04-27 22:13:07.859 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 6800]: Loss Value: 3.225827217102051\n",
      "2021-04-27 22:13:08.342 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 6900]: Loss Value: 3.3045105934143066\n",
      "2021-04-27 22:13:08.827 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 7000]: Loss Value: 3.2844343185424805\n",
      "2021-04-27 22:13:09.325 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 7100]: Loss Value: 3.3257906436920166\n",
      "2021-04-27 22:13:09.820 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 7200]: Loss Value: 3.1572606563568115\n",
      "2021-04-27 22:13:10.300 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 7300]: Loss Value: 3.241240978240967\n",
      "2021-04-27 22:13:10.780 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 7400]: Loss Value: 3.168236494064331\n",
      "2021-04-27 22:13:11.260 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 7500]: Loss Value: 3.1153578758239746\n",
      "2021-04-27 22:13:11.740 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 7600]: Loss Value: 3.346665382385254\n",
      "2021-04-27 22:13:12.219 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 7700]: Loss Value: 3.336027145385742\n",
      "2021-04-27 22:13:12.696 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 7800]: Loss Value: 3.1708996295928955\n",
      "2021-04-27 22:13:13.173 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 7900]: Loss Value: 3.131657123565674\n",
      "2021-04-27 22:13:13.653 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 8000]: Loss Value: 3.2623493671417236\n",
      "2021-04-27 22:13:14.133 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 8100]: Loss Value: 3.3221170902252197\n",
      "2021-04-27 22:13:14.617 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 8200]: Loss Value: 3.3794708251953125\n",
      "2021-04-27 22:13:15.100 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 8300]: Loss Value: 3.215207815170288\n",
      "2021-04-27 22:13:15.586 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 8400]: Loss Value: 3.232208728790283\n",
      "2021-04-27 22:13:16.068 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 8500]: Loss Value: 3.202695369720459\n",
      "2021-04-27 22:13:16.557 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 8600]: Loss Value: 3.1616673469543457\n",
      "2021-04-27 22:13:17.036 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 8700]: Loss Value: 3.244591236114502\n",
      "2021-04-27 22:13:17.517 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 8800]: Loss Value: 3.1573102474212646\n",
      "2021-04-27 22:13:18.001 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 8900]: Loss Value: 3.2627756595611572\n",
      "2021-04-27 22:13:18.515 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 9000]: Loss Value: 3.221776247024536\n",
      "2021-04-27 22:13:19.010 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 9100]: Loss Value: 3.232529401779175\n",
      "2021-04-27 22:13:19.507 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 9200]: Loss Value: 3.3287599086761475\n",
      "2021-04-27 22:13:19.985 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 9300]: Loss Value: 3.3757643699645996\n",
      "2021-04-27 22:13:20.465 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 9400]: Loss Value: 3.1958961486816406\n",
      "2021-04-27 22:13:20.941 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 9500]: Loss Value: 3.2414610385894775\n",
      "2021-04-27 22:13:21.417 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 9600]: Loss Value: 3.141997814178467\n",
      "2021-04-27 22:13:21.897 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 9700]: Loss Value: 3.249204397201538\n",
      "2021-04-27 22:13:22.384 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 9800]: Loss Value: 3.1930220127105713\n",
      "2021-04-27 22:13:22.941 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 9900]: Loss Value: 3.3186376094818115\n",
      "2021-04-27 22:13:23.424 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 10000]: Loss Value: 3.1457207202911377\n",
      "2021-04-27 22:13:23.900 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 10100]: Loss Value: 3.2577719688415527\n",
      "2021-04-27 22:13:24.384 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 10200]: Loss Value: 3.241025447845459\n",
      "2021-04-27 22:13:24.888 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 10300]: Loss Value: 3.1174793243408203\n",
      "2021-04-27 22:13:25.366 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 10400]: Loss Value: 3.259161949157715\n",
      "2021-04-27 22:13:25.855 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 10500]: Loss Value: 3.188753128051758\n",
      "2021-04-27 22:13:26.330 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 10600]: Loss Value: 3.238661527633667\n",
      "2021-04-27 22:13:26.806 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 10700]: Loss Value: 3.269090414047241\n",
      "2021-04-27 22:13:27.291 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 10800]: Loss Value: 3.236976385116577\n",
      "2021-04-27 22:13:27.771 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 10900]: Loss Value: 3.2634384632110596\n",
      "2021-04-27 22:13:28.250 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 11000]: Loss Value: 3.2407386302948\n",
      "2021-04-27 22:13:28.763 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 11100]: Loss Value: 3.283195972442627\n",
      "2021-04-27 22:13:29.245 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 11200]: Loss Value: 3.2293951511383057\n",
      "2021-04-27 22:13:29.724 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 11300]: Loss Value: 3.1744587421417236\n",
      "2021-04-27 22:13:30.224 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 11400]: Loss Value: 3.2197682857513428\n",
      "2021-04-27 22:13:30.698 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 11500]: Loss Value: 3.2276666164398193\n",
      "2021-04-27 22:13:31.174 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 11600]: Loss Value: 3.1607327461242676\n",
      "2021-04-27 22:13:31.658 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 11700]: Loss Value: 3.239694833755493\n",
      "2021-04-27 22:13:32.135 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 11800]: Loss Value: 3.2762205600738525\n",
      "2021-04-27 22:13:32.610 | INFO     | __main__:<module>:23 - [Epoch 2] [Step 11900]: Loss Value: 3.2051022052764893\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 200\n",
    "model = SkipGram(len(word_tokens), embedding_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "print_frequency = 100\n",
    "steps = 0\n",
    "epochs = 2\n",
    "\n",
    "batch_size = 32\n",
    "window = 5\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    logger.info(f\"Started epoch {epoch}\")\n",
    "    for x, y in generate_batches(words, batch_size, window):\n",
    "        steps += 1\n",
    "        inputs, targets = torch.LongTensor(x), torch.LongTensor(y)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if steps % print_frequency == 0:\n",
    "            logger.info(f\"[Epoch {epoch}] [Step {steps}]: Loss Value: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-junction",
   "metadata": {},
   "source": [
    "# Test word similarity - Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "foster-handy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said : ['would' 'go' 'frodo' 'sam' 'came']\n",
      "frodo : ['said' 'stood' 'danced' 'great' 'would']\n",
      "sam : ['said' 'drew' 'shadow' 'go' 'marshes']\n",
      "came : ['said' 'yet' 'look' 'osgiliath' 'without']\n",
      "great : ['frodo' 'said' 'gave' 'way' 'hobbits']\n",
      "could : ['arm' 'lost' 'way' 'go' 'greater']\n",
      "would : ['said' 'frodo' 'indeed' 'fear' 'way']\n",
      "long : ['seemed' 'corner' 'peril' 'tinúviel' 'meriadoc']\n",
      "gandalf : ['said' 'go' 'river' 'minas' 'perish']\n",
      "like : ['city' 'shining' 'north' 'gave' 'servants']\n",
      "go : ['said' 'reek' 'decided' 'sam' 'gandalf']\n",
      "back : ['seeing' 'think' 'said' 'lord' 'beside']\n",
      "away : ['think' 'sam' 'tongue' 'said' 'company']\n",
      "still : ['see' 'balin' 'said' 'horseman' 'warn']\n",
      "us : ['wide' 'saw' 'said' 'place' 'loud']\n",
      "many : ['river' 'master' 'halt' 'slain' 'quite']\n",
      "see : ['still' 'cry' 'said' 'wish' 'fear']\n",
      "upon : ['play' 'frodo' 'go' 'leaf' 'course']\n",
      "far : ['said' 'empty' 'nowhere' 'get' 'seemed']\n",
      "last : ['éowyn' 'lawn' 'says' 'red' 'saruman']\n",
      "dark : ['said' 'might' 'sam' 'fly' 'deadly']\n",
      "yet : ['marching' 'came' 'steel' 'mordor' 'stream']\n",
      "old : ['sort' 'passed' 'spoken' 'hope' 'much']\n",
      "way : ['eyes' 'go' 'said' 'would' 'could']\n",
      "aragorn : ['sam' 'knowing' 'bells' 'sought' 'laughing']\n",
      "went : ['nearer' 'guest' 'hobbiton' 'limbs' 'said']\n",
      "must : ['nine' 'soon' 'seemed' 'deep' 'said']\n",
      "seemed : ['said' 'long' 'hand' 'seen' 'far']\n",
      "time : ['`is' 'ever' 'snowmane' 'packs' 'frodo']\n",
      "pippin : ['lord' 'strider' 'fierce' 'begged' 'bless']\n",
      "shall : ['twenty' 'fierce' 'various' 'grishnákh' 'gandalf']\n",
      "even : ['much' 'frodo' 'said' 'sam' 'spot']\n",
      "may : ['together' 'see' 'sunlight' 'try' 'great']\n",
      "know : ['frodo' 'way' 'try' 'night' 'halted']\n",
      "well : ['destroy' 'family' 'pleasant' 'willow' 'said']\n",
      "looked : ['airs' 'fear' 'warned' 'bird' 'almost']\n",
      "eyes : ['way' 'sam' 'stars' 'said' 'came']\n",
      "merry : ['way' 'world' 'guards' 'walking' 'leagues']\n",
      "little : ['dead' 'written' 'gift' 'cousin' 'asleep']\n",
      "light : ['mists' 'could' 'knock' 'feeling' 'pretty']\n",
      "hobbits : ['refreshed' 'great' 'even' 'shriek' 'back']\n",
      "road : ['sang' 'could' 'stand' 'nightfall' 'birthday']\n",
      "thought : ['start' 'said' 'frodo' 'fear' 'yelled']\n",
      "saw : ['us' 'fair' 'mantle' 'said' 'queer']\n",
      "say : ['groped' 'fumes' 'cross-roads' 'silent' 'told']\n",
      "behind : ['easily' 'taller' 'frodo' 'wholly' 'told']\n",
      "stood : ['frodo' 'gave' 'battle' 'thing' 'paused']\n",
      "night : ['halted' 'said' 'pointing' 'attention' 'sat']\n",
      "hand : ['seemed' '`we' 'faint' 'wish' 'théoden']\n",
      "heard : ['already' 'view' 'hoping' 'roaring' 'miss']\n"
     ]
    }
   ],
   "source": [
    "embedding_weights = model.embed.weight\n",
    "embedding_norms = embedding_weights.pow(2).sum(dim=1).sqrt()\n",
    "normed_embedding = (embedding_weights.t() / embedding_norms).t()\n",
    "\n",
    "exmaple_count = 50\n",
    "random_set = np.arange(50)\n",
    "examples = torch.LongTensor(random_set)\n",
    "example_words = [unique_words[token] for token in examples]\n",
    "example_embeddings = model.embed(examples)\n",
    "similarities = torch.mm(example_embeddings, normed_embedding.t())\n",
    "\n",
    "_, top_k_similar = similarities.topk(6)\n",
    "similar = np.array(unique_words)[top_k_similar.numpy()][:, 1:]\n",
    "\n",
    "for i, word in enumerate(example_words):\n",
    "    print(f\"{word} : {similar[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "caroline-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = list(word_frequencies.values())\n",
    "frequencies.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "funky-battlefield",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_factor = .75\n",
    "smoothed_dist = np.power(frequencies, unigram_factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "specialized-highland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(words, batch_size, noise_count, window):\n",
    "    batch_count = len(words) // batch_size\n",
    "    words = words[:(batch_size * batch_count)]\n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        target, context = [], []\n",
    "        batch = words[idx:idx + batch_size]\n",
    "        for i, word in enumerate(batch):\n",
    "            word_token = word_tokens[word]\n",
    "            context_tokens = generate_word_context(i, words, window)\n",
    "            target.extend([word_token] * len(context_tokens))\n",
    "            context.extend(context_tokens)\n",
    "        yield np.array(target), np.array(context), np.random.choice(\n",
    "            len(noise_dist), size=len(target) * noise_count, replace=True, p=noise_dist).reshape(len(target), noise_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "infectious-undergraduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = generate_batches(words, 10, 5, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "russian-passing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramNegativeSampling(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super().__init__()\n",
    "        self.input_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.output_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.input_embedding.weight.data.uniform_(-1, 1)\n",
    "        self.output_embedding.weight.data.uniform_(-1, 1)\n",
    "    \n",
    "    def forward(self, target, context, noise):\n",
    "        target_embed = self.input_embedding(target)\n",
    "        context_embed = self.output_embedding(context)\n",
    "        noise_embed = self.output_embedding(noise)\n",
    "        return target_embed, context_embed, noise_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "first-humanitarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSamplingLoss(nn.Module):\n",
    "    def forward(self, target, context, noise):\n",
    "        batch_size, embed_size = target.shape\n",
    "        target = target.view(batch_size, embed_size, 1)\n",
    "        context = context.view(batch_size, 1, embed_size)\n",
    "       \n",
    "        output_loss = torch.bmm(context, target).sigmoid().log().squeeze()\n",
    "        noise_loss = torch.bmm(noise.neg(), target).sigmoid().log().squeeze().sum(1)\n",
    "        return -(output_loss + noise_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-value",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "difficult-metallic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-27 22:33:56.640 | INFO     | __main__:<module>:15 - Started epoch 1\n",
      "2021-04-27 22:33:57.013 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 100]: Loss Value: 10.910740852355957\n",
      "2021-04-27 22:33:57.357 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 200]: Loss Value: 10.109518051147461\n",
      "2021-04-27 22:33:57.693 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 300]: Loss Value: 9.514039039611816\n",
      "2021-04-27 22:33:58.048 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 400]: Loss Value: 10.194342613220215\n",
      "2021-04-27 22:33:58.384 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 500]: Loss Value: 9.873336791992188\n",
      "2021-04-27 22:33:58.734 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 600]: Loss Value: 9.31712532043457\n",
      "2021-04-27 22:33:59.080 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 700]: Loss Value: 10.077784538269043\n",
      "2021-04-27 22:33:59.432 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 800]: Loss Value: 8.74406623840332\n",
      "2021-04-27 22:33:59.763 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 900]: Loss Value: 9.031476020812988\n",
      "2021-04-27 22:34:00.097 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 1000]: Loss Value: 8.240180969238281\n",
      "2021-04-27 22:34:00.433 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 1100]: Loss Value: 7.7997050285339355\n",
      "2021-04-27 22:34:00.766 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 1200]: Loss Value: 7.831036567687988\n",
      "2021-04-27 22:34:01.101 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 1300]: Loss Value: 7.9269118309021\n",
      "2021-04-27 22:34:01.433 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 1400]: Loss Value: 7.117364883422852\n",
      "2021-04-27 22:34:01.785 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 1500]: Loss Value: 7.2817583084106445\n",
      "2021-04-27 22:34:02.149 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 1600]: Loss Value: 7.563261985778809\n",
      "2021-04-27 22:34:02.484 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 1700]: Loss Value: 6.072746753692627\n",
      "2021-04-27 22:34:02.815 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 1800]: Loss Value: 5.629329681396484\n",
      "2021-04-27 22:34:03.164 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 1900]: Loss Value: 6.343369960784912\n",
      "2021-04-27 22:34:03.505 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 2000]: Loss Value: 5.4693922996521\n",
      "2021-04-27 22:34:03.838 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 2100]: Loss Value: 5.433836460113525\n",
      "2021-04-27 22:34:04.214 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 2200]: Loss Value: 5.570262908935547\n",
      "2021-04-27 22:34:04.553 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 2300]: Loss Value: 5.324697971343994\n",
      "2021-04-27 22:34:04.892 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 2400]: Loss Value: 4.844237327575684\n",
      "2021-04-27 22:34:05.230 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 2500]: Loss Value: 4.033240795135498\n",
      "2021-04-27 22:34:05.569 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 2600]: Loss Value: 4.420095920562744\n",
      "2021-04-27 22:34:05.932 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 2700]: Loss Value: 4.94597053527832\n",
      "2021-04-27 22:34:06.270 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 2800]: Loss Value: 4.712769985198975\n",
      "2021-04-27 22:34:06.606 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 2900]: Loss Value: 3.526085615158081\n",
      "2021-04-27 22:34:06.941 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 3000]: Loss Value: 3.7893762588500977\n",
      "2021-04-27 22:34:07.277 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 3100]: Loss Value: 2.825650215148926\n",
      "2021-04-27 22:34:07.612 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 3200]: Loss Value: 3.1940507888793945\n",
      "2021-04-27 22:34:07.944 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 3300]: Loss Value: 2.545666217803955\n",
      "2021-04-27 22:34:08.281 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 3400]: Loss Value: 2.140200614929199\n",
      "2021-04-27 22:34:08.616 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 3500]: Loss Value: 3.2279322147369385\n",
      "2021-04-27 22:34:08.950 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 3600]: Loss Value: 2.0213868618011475\n",
      "2021-04-27 22:34:09.287 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 3700]: Loss Value: 2.2472727298736572\n",
      "2021-04-27 22:34:09.654 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 3800]: Loss Value: 2.3530287742614746\n",
      "2021-04-27 22:34:10.020 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 3900]: Loss Value: 2.187915325164795\n",
      "2021-04-27 22:34:10.362 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 4000]: Loss Value: 2.141374349594116\n",
      "2021-04-27 22:34:10.706 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 4100]: Loss Value: 2.01680064201355\n",
      "2021-04-27 22:34:11.047 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 4200]: Loss Value: 2.5354084968566895\n",
      "2021-04-27 22:34:11.387 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 4300]: Loss Value: 1.496715784072876\n",
      "2021-04-27 22:34:11.722 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 4400]: Loss Value: 1.610618233680725\n",
      "2021-04-27 22:34:12.062 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 4500]: Loss Value: 1.4703582525253296\n",
      "2021-04-27 22:34:12.404 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 4600]: Loss Value: 1.2463462352752686\n",
      "2021-04-27 22:34:12.812 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 4700]: Loss Value: 1.3036305904388428\n",
      "2021-04-27 22:34:13.164 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 4800]: Loss Value: 1.126956582069397\n",
      "2021-04-27 22:34:13.501 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 4900]: Loss Value: 1.241490364074707\n",
      "2021-04-27 22:34:13.839 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 5000]: Loss Value: 0.7978808879852295\n",
      "2021-04-27 22:34:14.178 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 5100]: Loss Value: 1.2082505226135254\n",
      "2021-04-27 22:34:14.516 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 5200]: Loss Value: 1.1972346305847168\n",
      "2021-04-27 22:34:14.854 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 5300]: Loss Value: 0.8280758261680603\n",
      "2021-04-27 22:34:15.193 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 5400]: Loss Value: 1.2125496864318848\n",
      "2021-04-27 22:34:15.529 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 5500]: Loss Value: 1.0313472747802734\n",
      "2021-04-27 22:34:15.864 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 5600]: Loss Value: 0.9181322455406189\n",
      "2021-04-27 22:34:16.201 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 5700]: Loss Value: 0.8072066307067871\n",
      "2021-04-27 22:34:16.536 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 5800]: Loss Value: 1.1981209516525269\n",
      "2021-04-27 22:34:16.880 | INFO     | __main__:<module>:25 - [Epoch 1] [Step 5900]: Loss Value: 0.7068614363670349\n",
      "2021-04-27 22:34:17.110 | INFO     | __main__:<module>:15 - Started epoch 2\n",
      "2021-04-27 22:34:17.241 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 6000]: Loss Value: 1.0036606788635254\n",
      "2021-04-27 22:34:17.588 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 6100]: Loss Value: 1.2700426578521729\n",
      "2021-04-27 22:34:17.924 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 6200]: Loss Value: 0.7556596994400024\n",
      "2021-04-27 22:34:18.262 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 6300]: Loss Value: 0.5957699418067932\n",
      "2021-04-27 22:34:18.599 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 6400]: Loss Value: 0.5501132011413574\n",
      "2021-04-27 22:34:18.935 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 6500]: Loss Value: 0.5500856041908264\n",
      "2021-04-27 22:34:19.277 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 6600]: Loss Value: 0.5596446394920349\n",
      "2021-04-27 22:34:19.631 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 6700]: Loss Value: 0.8539056181907654\n",
      "2021-04-27 22:34:20.014 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 6800]: Loss Value: 0.6358557343482971\n",
      "2021-04-27 22:34:20.363 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 6900]: Loss Value: 0.5366457104682922\n",
      "2021-04-27 22:34:20.703 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 7000]: Loss Value: 0.498048335313797\n",
      "2021-04-27 22:34:21.041 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 7100]: Loss Value: 0.4214269816875458\n",
      "2021-04-27 22:34:21.377 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 7200]: Loss Value: 0.6718466877937317\n",
      "2021-04-27 22:34:21.714 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 7300]: Loss Value: 0.4922259449958801\n",
      "2021-04-27 22:34:22.052 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 7400]: Loss Value: 0.634701132774353\n",
      "2021-04-27 22:34:22.390 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 7500]: Loss Value: 0.4966135323047638\n",
      "2021-04-27 22:34:22.727 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 7600]: Loss Value: 0.5429094433784485\n",
      "2021-04-27 22:34:23.065 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 7700]: Loss Value: 0.47007498145103455\n",
      "2021-04-27 22:34:23.402 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 7800]: Loss Value: 0.5837371349334717\n",
      "2021-04-27 22:34:23.739 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 7900]: Loss Value: 0.7242175340652466\n",
      "2021-04-27 22:34:24.075 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 8000]: Loss Value: 0.5067881941795349\n",
      "2021-04-27 22:34:24.410 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 8100]: Loss Value: 0.5129631161689758\n",
      "2021-04-27 22:34:24.750 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 8200]: Loss Value: 0.5152245163917542\n",
      "2021-04-27 22:34:25.095 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 8300]: Loss Value: 0.3720036745071411\n",
      "2021-04-27 22:34:25.448 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 8400]: Loss Value: 0.4713175296783447\n",
      "2021-04-27 22:34:25.791 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 8500]: Loss Value: 0.5782727003097534\n",
      "2021-04-27 22:34:26.129 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 8600]: Loss Value: 0.39661476016044617\n",
      "2021-04-27 22:34:26.466 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 8700]: Loss Value: 0.478759765625\n",
      "2021-04-27 22:34:26.803 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 8800]: Loss Value: 0.38654249906539917\n",
      "2021-04-27 22:34:27.141 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 8900]: Loss Value: 0.38046708703041077\n",
      "2021-04-27 22:34:27.479 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 9000]: Loss Value: 0.48647692799568176\n",
      "2021-04-27 22:34:27.812 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 9100]: Loss Value: 0.49171075224876404\n",
      "2021-04-27 22:34:28.147 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 9200]: Loss Value: 0.5069680213928223\n",
      "2021-04-27 22:34:28.479 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 9300]: Loss Value: 0.38412702083587646\n",
      "2021-04-27 22:34:28.813 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 9400]: Loss Value: 0.4001336693763733\n",
      "2021-04-27 22:34:29.147 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 9500]: Loss Value: 0.4653262794017792\n",
      "2021-04-27 22:34:29.493 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 9600]: Loss Value: 0.3550851047039032\n",
      "2021-04-27 22:34:29.830 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 9700]: Loss Value: 0.4753030836582184\n",
      "2021-04-27 22:34:30.203 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 9800]: Loss Value: 0.33372750878334045\n",
      "2021-04-27 22:34:30.541 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 9900]: Loss Value: 0.4493457078933716\n",
      "2021-04-27 22:34:30.875 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 10000]: Loss Value: 0.2898631989955902\n",
      "2021-04-27 22:34:31.219 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 10100]: Loss Value: 0.35531672835350037\n",
      "2021-04-27 22:34:31.569 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 10200]: Loss Value: 0.3195957541465759\n",
      "2021-04-27 22:34:31.901 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 10300]: Loss Value: 0.33091044425964355\n",
      "2021-04-27 22:34:32.234 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 10400]: Loss Value: 0.2807883322238922\n",
      "2021-04-27 22:34:32.601 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 10500]: Loss Value: 0.31958726048469543\n",
      "2021-04-27 22:34:32.933 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 10600]: Loss Value: 0.45263177156448364\n",
      "2021-04-27 22:34:33.282 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 10700]: Loss Value: 0.4942764341831207\n",
      "2021-04-27 22:34:33.618 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 10800]: Loss Value: 0.3109079599380493\n",
      "2021-04-27 22:34:33.953 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 10900]: Loss Value: 0.3218497633934021\n",
      "2021-04-27 22:34:34.294 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 11000]: Loss Value: 0.4277755618095398\n",
      "2021-04-27 22:34:34.630 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 11100]: Loss Value: 0.35741275548934937\n",
      "2021-04-27 22:34:34.964 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 11200]: Loss Value: 0.3475540280342102\n",
      "2021-04-27 22:34:35.299 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 11300]: Loss Value: 0.2982448935508728\n",
      "2021-04-27 22:34:35.634 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 11400]: Loss Value: 0.37909790873527527\n",
      "2021-04-27 22:34:35.975 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 11500]: Loss Value: 0.3607322573661804\n",
      "2021-04-27 22:34:36.309 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 11600]: Loss Value: 0.31772828102111816\n",
      "2021-04-27 22:34:36.659 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 11700]: Loss Value: 0.3931396007537842\n",
      "2021-04-27 22:34:36.999 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 11800]: Loss Value: 0.5264931917190552\n",
      "2021-04-27 22:34:37.344 | INFO     | __main__:<module>:25 - [Epoch 2] [Step 11900]: Loss Value: 0.32565996050834656\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 150\n",
    "model = SkipGramNegativeSampling(len(word_tokens), embedding_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = NegativeSamplingLoss()\n",
    "\n",
    "print_frequency = 100\n",
    "steps = 0\n",
    "epochs = 2\n",
    "\n",
    "batch_size = 32\n",
    "window = 5\n",
    "noise_words = 5\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    logger.info(f\"Started epoch {epoch}\")\n",
    "    for x, y, n in generate_batches(words, batch_size=batch_size, noise_count=noise_words, window=window):\n",
    "        steps += 1\n",
    "        target, context, noise = torch.LongTensor(x), torch.LongTensor(y), torch.LongTensor(n)\n",
    "        target_embed, context_embed, noise_embed = model(target, context, noise)\n",
    "        loss = loss_function(target_embed, context_embed, noise_embed)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if steps % print_frequency == 0:\n",
    "            logger.info(f\"[Epoch {epoch}] [Step {steps}]: Loss Value: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "contemporary-switch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said : ['like' 'came' 'looked' 'stood' 'back']\n",
      "frodo : ['came' 'many' 'saruman' 'time' 'though']\n",
      "sam : ['age' 'lord' 'black' 'us' 'like']\n",
      "came : ['said' 'frodo' 'stood' 'ring' 'sat']\n",
      "great : ['black' 'could' 'long' 'said' 'must']\n",
      "could : ['know' 'great' 'night' 'us' 'behind']\n",
      "would : ['fear' 'upon' 'entered' 'great' 'long']\n",
      "long : ['road' '–' 'great' 'hand' 'away']\n",
      "gandalf : ['yet' 'towards' 'like' 'back' 'upon']\n",
      "like : ['said' 'well' 'back' 'mist' 'night']\n",
      "go : ['said' 'aragorn' 'lie' 'frodo' 'flat']\n",
      "back : ['said' 'like' 'gandalf' 'looked' 'journey']\n",
      "away : ['yet' 'long' 'much' 'chief' 'go']\n",
      "still : ['shire' 'night' 'must' 'windows' 'world']\n",
      "us : ['hand' 'sam' 'wood' 'could' 'set']\n",
      "many : ['frodo' 'said' 'sight' 'sky' 'could']\n",
      "see : ['seemed' 'wind' 'soon' 'always' 'sam']\n",
      "upon : ['said' 'looking' 'gandalf' 'looked' 'like']\n",
      "far : ['tall' 'lord' 'set' 'nothing' 'darkness']\n",
      "last : ['middle-earth' 'way' 'towards' 'speaking' 'shall']\n",
      "dark : ['light' 'grey' 'upon' 'hills' 'air']\n",
      "yet : ['gandalf' 'king' 'fair' 'away' 'shall']\n",
      "old : ['fear' 'whatever' 'maybe' 'cried' 'looked']\n",
      "way : ['looked' 'hobbits' 'last' 'said' 'showed']\n",
      "aragorn : ['trees' 'go' 'things' 'arms' 'said']\n",
      "went : ['answered' 'never' 'like' 'ring' 'kept']\n",
      "must : ['tell' 'great' 'still' 'heard' 'even']\n",
      "seemed : ['see' 'land' 'time' 'made' 'passed']\n",
      "time : ['frodo' 'black' 'seemed' 'find' 'shire']\n",
      "pippin : ['gate' 'came' 'hobbits' 'edge' 'every']\n",
      "shall : ['yet' 'said' 'king' 'merry' 'isengard']\n",
      "even : ['green' 'said' 'shire' 'road' 'river']\n",
      "may : ['moment' 'eye' 'still' 'black' 'like']\n",
      "know : ['could' 'said' 'clear' 'like' 'great']\n",
      "well : ['like' 'said' 'pale' 'red' 'go']\n",
      "looked : ['said' 'way' 'came' 'sky' 'upon']\n",
      "eyes : ['towards' 'gandalf' 'upon' 'city' 'air']\n",
      "merry : ['lord' 'said' 'shall' 'might' 'could']\n",
      "little : ['night' 'great' 'doubt' 'hobbits' 'flash']\n",
      "light : ['said' 'need' 'dark' 'upon' 'fire']\n",
      "hobbits : ['way' 'pippin' 'upon' 'turned' 'back']\n",
      "road : ['long' 'said' 'precious' 'side' 'even']\n",
      "thought : ['call' 'turned' 'faces' 'put' 'behold']\n",
      "saw : ['seems' 'despair' 'see' 'still' 'found']\n",
      "say : ['far' 'gandalf' 'guess' 'came' 'ring']\n",
      "behind : ['towards' 'sound' 'could' 'drawn' 'grey']\n",
      "stood : ['said' 'came' 'land' 'looked' 'back']\n",
      "night : ['like' 'still' 'could' 'great' 'little']\n",
      "hand : ['cry' 'long' 'us' 'made' 'guess']\n",
      "heard : ['must' 'silver' 'long' 'came' 'water']\n",
      "think : ['said' 'fear' 'go' 'need' 'enough']\n",
      "ever : ['gone' 'seen' 'ring' 'four' 'talking']\n",
      "passed : ['hobbit' 'light' 'came' 'stood' 'set']\n",
      "good : ['path' 'soon' 'merry' 'got' 'let']\n",
      "black : ['gimli' 'towards' 'great' 'sam' 'without']\n",
      "though : ['frodo' 'fair' 'air' 'merry' 'passed']\n",
      "day : ['said' 'made' 'friend' 'fell' 'gandalf']\n",
      "left : ['small' 'green' 'strange' 'see' 'even']\n",
      "ring : ['came' 'frodo' 'ever' 'anyway' 'way']\n",
      "made : ['said' 'hand' 'far' 'surely' 'back']\n",
      "much : ['said' 'yet' 'away' 'beyond' 'things']\n",
      "let : ['said' 'heart' 'soon' 'yet' 'good']\n",
      "things : ['aragorn' 'cried' 'much' 'answer' 'full']\n",
      "first : ['light' 'say' 'anyway' 'said' 'make']\n",
      "land : ['said' 'stood' 'enough' 'seems' 'answer']\n",
      "suddenly : ['sun' 'faded' 'took' 'great' 'step']\n",
      "white : ['`i' 'go' 'said' 'fear' 'cried']\n",
      "going : ['gollum' 'still' 'even' 'mark' 'gandalf']\n",
      "two : ['shire' 'like' 'sméagol' 'long' 'still']\n",
      "lord : ['merry' 'sam' 'far' 'beside' 'valley']\n",
      "found : ['saw' 'would' 'land' 'already' 'said']\n",
      "gollum : ['merry' 'going' 'host' 'back' 'almost']\n",
      "master : ['hear' 'watched' 'grief' 'answered' 'drew']\n",
      "trees : ['aragorn' 'like' 'still' 'great' 'two']\n",
      "fell : ['rohirrim' 'day' 'running' 'growing' 'might']\n",
      "king : ['yet' 'shall' 'dream' 'said' 'mind']\n",
      "turned : ['thought' 'hobbits' 'eyes' 'gates' 'old']\n",
      "gimli : ['black' 'frodo' 'us' 'running' 'play']\n",
      "– : ['long' 'came' 'gandalf' 'stone' 'yet']\n",
      "side : ['set' 'road' 'frodo' 'forth' 'south']\n",
      "hope : ['return' 'spoke' 'woke' 'rode' 'king']\n",
      "soon : ['lying' 'see' 'still' 'upon' 'old']\n",
      "seen : ['feet' 'ever' 'company' 'quickly' 'battle']\n",
      "lay : ['sent' 'dark' 'passed' 'may' 'stand']\n",
      "deep : ['straight' 'going' 'council' 'yet' 'slow']\n",
      "beyond : ['much' 'lands' 'said' 'west' 'long']\n",
      "get : ['eye' 'far' 'flash' 'asked' 'sleep']\n",
      "cried : ['said' 'things' 'old' 'hearts' 'white']\n",
      "bilbo : ['elrond' 'came' 'fire' 'began' 'sound']\n",
      "find : ['time' 'farewell' 'great' 'open' 'upon']\n",
      "days : ['know' 'frodo' 'said' 'south' 'young']\n",
      "never : ['well' 'went' 'stand' 'wonder' 'inside']\n",
      "water : ['dark' 'heard' 'gimli' 'return' 'road']\n",
      "set : ['side' 'far' 'us' 'knees' 'sam']\n",
      "feet : ['seen' 'make' 'ever' 'heart' 'behind']\n",
      "right : ['said' 'looked' 'upon' 'like' 'woke']\n",
      "indeed : ['bid' 'effort' 'gondor' 'walk' 'frodo']\n",
      "grey : ['known' 'dark' 'upon' 'behind' 'people']\n",
      "legolas : ['great' 'gondor' 'see' 'lads' 'hurry']\n",
      "orcs : ['north' 'evil' 'barren' 'see' 'land']\n"
     ]
    }
   ],
   "source": [
    "embedding_weights = model.input_embedding.weight\n",
    "embedding_norms = embedding_weights.pow(2).sum(dim=1).sqrt()\n",
    "normed_embedding = (embedding_weights.t() / embedding_norms).t()\n",
    "\n",
    "examples = torch.LongTensor(np.arange(100))\n",
    "example_words = [unique_words[token] for token in examples]\n",
    "example_embeddings = model.input_embedding(examples)\n",
    "similarities = torch.mm(example_embeddings, normed_embedding.t())\n",
    "\n",
    "_, top_k_similar = similarities.topk(6)\n",
    "similar = np.array(unique_words)[top_k_similar.numpy()][:, 1:]\n",
    "\n",
    "for i, word in enumerate(example_words):\n",
    "    print(f\"{word} : {similar[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-tournament",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-arrival",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
